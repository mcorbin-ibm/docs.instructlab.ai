{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the \ud83d\udc36 InstructLab Project","text":"<p>InstructLab is a model-agnostic open source AI project that facilitates contributions to Large Language Models (LLMs).</p> <p>We are on a mission to let anyone shape generative AI by enabling contributed updates to existing LLMs in an accessible way.</p> <p>Our community welcomes all those who would like to help us enable everyone to shape the future of generative AI.</p>"},{"location":"#why-instructlab","title":"Why InstructLab","text":"<p>There are many projects rapidly embracing and extending permissively licensed AI models, but they are faced with three main challenges:</p> <ul> <li>Contribution to LLMs is not possible directly. They show up as forks, which forces consumers to choose a \u201cbest-fit\u201d model that isn\u2019t easily extensible. Also, the forks are expensive for model creators to maintain.</li> <li>The ability to contribute ideas is limited by a lack of AI/ML expertise. One has to learn how to fork, train, and refine models to see their idea move forward. This is a high barrier to entry.</li> <li>There is no direct community governance or best practice around review, curation, and distribution of forked models.</li> </ul> <p>InstructLab is here to solve these problems.</p> <p>The project enables community contributors to add additional \"skills\" or \"knowledge\" to a particular model.</p> <p>InstructLab's model-agnostic technology gives model upstreams with sufficient infrastructure resources the ability to create regular builds of their open source licensed models not by rebuilding and retraining the entire model but by composing new skills into it.</p> <p>Take a look at \"lab-enhanced\" models on the InstructLab Hugging Face page.</p>"},{"location":"#requirements","title":"\ud83d\udccb Requirements","text":"<ul> <li>\ud83c\udf4e Apple M1/M2/M3 Mac or \ud83d\udc27 Linux system (tested on Fedora).   We anticipate support for more operating systems in the future.</li> <li>C++ compiler</li> <li>Python 3.10 or Python 3.11</li> <li>Approximately 60GB disk space (entire process)</li> </ul> <p>Note</p> <p>Python 3.12 is currently not supported, because some dependencies don't work on Python 3.12, yet.</p> <p>Tip</p> <p>When installing the <code>ilab</code> CLI on macOS, you may have to run the <code>xcode-select --install</code> command, installing the required packages previously listed.</p>"},{"location":"cmb/","title":"About Community Model Build","text":"<p>TODO</p>"},{"location":"community/CODE_OF_CONDUCT/","title":"InstructLab - Code of Conduct and Covenant","text":""},{"location":"community/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"community/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or  advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic  address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a  professional setting</li> </ul>"},{"location":"community/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"community/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the Code of Conduct Committee. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p> <p>You can find the Code of Conduct Committee member email addresses listed here.</p>"},{"location":"community/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"community/CODE_OF_CONDUCT_COMMITTEE/","title":"Filing a Code of Conduct Violation Report","text":"<p>We take reports of violations to our project Code of Conduct with the utmost seriousness and will act upon them as quickly as possible.</p> <p>To report a Code of Conduct violation to our Code of Conduct Committee, you may reach out to us by email at coc@instructlab.ai. The email will be read and acted upon by our Code of Conduct Committee members.</p> <p>If you experience a Code of Conduct violation in our InstructLab Slack workspace, please follow the instructions in our moderation guide to get immediate help in Slack.</p> <p>As part of our follow up on your report, we would like to contact you for further discussion. If you would prefer not to engage beyond reporting the matter to the committee, please let us know that as part of your submission. We will respect your request.</p>"},{"location":"community/CODE_OF_CONDUCT_COMMITTEE/#code-of-conduct-committee-members","title":"Code of Conduct Committee Members","text":"<p>The current members of the CoCC are:</p> <ul> <li>Cara Delia, Red Hat</li> <li>Carol Chen, Red Hat</li> <li>Leslie Hawthorn, Red Hat</li> <li>JJ Ashgar, IBM</li> <li>Joe Sepi, IBM</li> <li>Maureen McElaney, IBM</li> </ul>"},{"location":"community/CONTRIBUTING/","title":"Contributing","text":"<p>\ud83d\udc4d\ud83c\udf89 First off, thank you for taking the time to contribute! \ud83c\udf89\ud83d\udc4d</p> <p>The following is a set of guidelines for contributing. These are just guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.</p>"},{"location":"community/CONTRIBUTING/#what-to-know-before-getting-started","title":"What to know before getting started","text":""},{"location":"community/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to a Code of Conduct. By participating, you are expected to uphold this code.</p> <p>Please report unacceptable behavior to one of the Code of Conduct Committee members.</p>"},{"location":"community/CONTRIBUTING/#related-repositories","title":"Related repositories","text":"<p>In addition to this repository, InstructLab has two related repositories:</p> <ul> <li><code>ilab</code> command-line interface (CLI) tool. This repository is responsible for the <code>ilab</code> command-line interface (CLI) tool.</li> <li>taxonomy tree. This repository is responsible for the taxonomy tree that allows you to create models tuned with your data.</li> </ul> <p>The following sections provide a general overview for contributing to any of the InstructLab repositories.</p>"},{"location":"community/CONTRIBUTING/#contributing-overview","title":"Contributing overview","text":"<p>Participating in the InstructLab project can come by contributing to any one of the repositories. The following workflow is designed to help you understand contribution best practices, and to help you begin your contribution journey. It will guide you through creating and picking up issues, working through them, having your work reviewed, and then getting your pull request merged.</p> <p>Help on open source projects is always welcome and there is always something that can be improved. For example, documentation (like the text you are reading now) can always use improvement, code can always be clarified, variables or functions can always be renamed or commented on, and there is always a need for more test coverage. If you see something that you think should be fixed, take ownership!</p> <p>To contribute code or documentation, please submit a pull request to the relevant repository. Note that contribution to any repository has its own set of requirements and expectations, and users should familiarize themselves with those expectations before contributing.</p>"},{"location":"community/CONTRIBUTING/#ilab-command-line-interface-cli-tool-repository","title":"ilab command-line interface (CLI) tool repository","text":"<p>We welcome contributions in the form of pull requests for documentation updates, code contributions and more. Prior to contribution, users should acquaint themselves with the <code>ilab</code> CLI repository contribution guide.</p> <p>To submit a pull request to the <code>ilab</code> CLI tool repository, see the pull request page.</p>"},{"location":"community/CONTRIBUTING/#taxonomy-repository","title":"Taxonomy repository","text":"<p>We welcome contributions in the form of pull requests for documentation updates, skills contributions, knowledge contributions and more. Prior to contribution, users should acquaint themselves with the taxonomy repository contribution guide.</p> <p>To submit a pull request to the taxonomy repository, see the pull request page.</p>"},{"location":"community/CONTRIBUTING/#community","title":"Community","text":"<p>We welcome contributions in the form of pull requests for documentation. To submit a pull request to the community repository, see the pull request page.</p>"},{"location":"community/CONTRIBUTING/#getting-started-with-contribution","title":"Getting started with contribution","text":"<p>The InstructLab project uses the Fork and Pull model for contribution that is common in many open source repositories; this entails multiple steps, including forking and cloning the repository, creating a pull request (PR), and more. For details on this process, check out The GitHub Workflow Guide from Kubernetes.</p> <p>After you have forked and cloned the repository, you can start the contribution process by looking at the issue trackers of the community repository, CLI repository, or the taxonomy repository. You can then pick up an issue by leaving a comment on said issue, and address the issue in a pull request (PR). Prior to submission, make sure that your changes pass formatting, linting, and unit tests. Additionally, all PRs must agree to the terms of Developer Certificate of Origin (DCO) by signing off your commits. Only PRs with commits signed off are accepted. If you didn't sign off your commits before creating the pull request, no worries, you can fix that after the fact. For more information about this process, see Developer Certificate of Origin (DCO).</p> <p>Then, you can submit the PR to be reviewed. In general, we follow the standard GitHub pull request process. Follow the provided template on your PR to include details about your pull request for the maintainers.</p> <p>[!IMPORTANT] If you are seeking to make a larger contribution, such as introducing a new feature or functionality, or refactoring a significant portion of the codebase to improve performance, readability, or maintainability, get in touch with us prior to starting. This helps ensure that your time is not wasted working on a change that the project developers will not accept into the codebase.</p>"},{"location":"community/CONTRIBUTING/#pull-request-review","title":"Pull request review","text":"<p>Once you've created a pull request (PR), maintainers will review your code and may make suggestions to fix before merging. It will be easier for your PR to receive reviews if you consider the criteria the reviewers follow while working. Remember to:</p> <ul> <li>Run tests locally and ensure that they pass</li> <li>Follow the project coding conventions</li> <li>Write detailed commit messages</li> <li>Break large changes into a logical series of smaller patches, which are easy to understand individually and combine to solve a broader issue</li> </ul> <p>For a list of the maintainers and triagers, see the MAINTAINERS.md page.</p>"},{"location":"community/CONTRIBUTING/#proposing-new-features","title":"Proposing new features","text":"<p>To propose a new feature, it's best to raise an issue in the appropriate repository:</p> <ul> <li>InstructLab CLI repository</li> <li>Taxonomy repository</li> </ul> <p>This way, features can be discussed with the project maintainers, ensuring that your time is not wasted working on a feature that the project developers will not accept into the codebase.</p>"},{"location":"community/CONTRIBUTING/#submitting-or-fixing-bugs","title":"Submitting or fixing bugs","text":"<p>To submit a new bug, raise an issue in the appropriate repository before creating a pull request. This ensures that the issue is appropriately tracked.</p> <p>To fix an existing bug, leave a comment on the issue that you are working on. Then, create a pull request and submit the pull request for review.</p>"},{"location":"community/CONTRIBUTING/#legal","title":"Legal","text":"<p>The following sections detail important legal information that should be viewed prior to contribution.</p>"},{"location":"community/CONTRIBUTING/#license-and-copyright","title":"License and Copyright","text":"<p>Distributed under the Apache License, Version 2.0.</p> <p>SPDX-License-Identifier: Apache-2.0</p> <p>If you would like to see the detailed LICENSE click here.</p>"},{"location":"community/CONTRIBUTING/#developer-certificate-of-origin-dco","title":"Developer Certificate of Origin (DCO)","text":"<p>We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions.</p> <p>We ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. If you set your <code>user.name</code> and <code>user.email</code> in your <code>git config</code> file, you can sign your commit automatically by using the following command:</p> <pre><code>git commit -s\n</code></pre> <p>The following example includes a <code>Signed-off-by:</code> line, which indicates that the submitter has accepted the DCO:</p> <pre><code>Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>We automatically verify that all commit messages contain a <code>Signed-off-by:</code> line with your email address.</p>"},{"location":"community/CONTRIBUTING/#useful-tools-for-doing-dco-signoffs","title":"Useful tools for doing DCO signoffs","text":"<p>There are a number of tools that make it easier for developers to manage DCO signoffs.</p> <ul> <li>DCO command line tool, which let's you do a single signoff for an entire repo ( https://github.com/coderanger/dco )</li> <li>GitHub UI integrations for adding the signoff automatically ( https://github.com/scottrigby/dco-gh-ui )</li> <li>Chrome - https://chrome.google.com/webstore/detail/dco-github-ui/onhgmjhnaeipfgacbglaphlmllkpoijo</li> <li>Firefox - https://addons.mozilla.org/en-US/firefox/addon/scott-rigby/?src=search</li> </ul>"},{"location":"community/CONTRIBUTING/#communication","title":"Communication","text":"<p>You can join the InstructLab Slack workspace to communicate with project maintainers and your fellow users.</p>"},{"location":"community/CONTRIBUTING/#additional-resources","title":"Additional resources","text":"<p>The following resources include additional information about each repository, such as setting up the environment, testing the environment, coding styles, etc.</p>"},{"location":"community/CONTRIBUTING/#ilab-cli-tool-additional-resources","title":"ilab CLI tool additional resources","text":"<ul> <li> <p><code>ilab</code> CLI tool README.md. This resources provides information about the <code>ilab</code> CLI tool, including an overview, getting started, training the model, submitting a pull request, etc.</p> </li> <li> <p><code>ilab</code> CLI tool CONTRIBUTING.md. This resources provides information about contributing to the <code>ilab</code> CLI tool repository, reporting bugs, testing, coding styles, etc.</p> </li> </ul>"},{"location":"community/CONTRIBUTING/#taxonomy-additional-resources","title":"Taxonomy additional resources","text":"<ul> <li> <p>Taxonomy README.md. This resource provides information about the taxonomy repository, including getting started, YAML examples for skills and knowledge pull requests, how to contribute, etc.</p> </li> <li> <p>Taxonomy CONTRIBUTING.md. This resource contains information and best practices for contributing to the taxonomy repository.</p> </li> </ul>"},{"location":"community/FAQ/","title":"InstructLab FAQ","text":"<p>Last updated: April 2024</p>"},{"location":"community/FAQ/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Document summary</li> <li>General FAQ</li> <li>What is InstructLab?</li> <li>What is LAB?</li> <li>How does InstructLab work?</li> <li>What are the goals of the InstructLab project?</li> <li>How can I contribute?</li> <li>I'm having problems with the <code>ilab</code> CLI tool. What should I do?</li> <li>Why should I contribute?</li> <li>What large language models (LLMs) am I contributing to through the InstructLab project?</li> <li>What is Merlinite-7b?</li> <li>What is Granite-7b-lab?</li> <li>What is a \u201cskill\u201d?</li> <li>What is \u201cknowledge\u201d?</li> <li>Is the project looking for certain types of skill contributions?</li> <li>What are the acceptance criteria for a skills submission?</li> <li>What are the acceptance criteria for a knowledge submission?</li> <li>How can I submit a skill or knowledge?</li> <li>What happens after you submit a pull request?</li> <li>How are submissions reviewed?</li> <li>How long will it take for my pull request to be reviewed?</li> <li>If my pull request is accepted, how long will it take for my changes to appear in the next model update?</li> <li>What is the software license for InstructLab?</li> <li>Am I required to license code submissions to InstructLab under the Apache 2.0 license?</li> <li>My contribution requires submitting data along with code. What data is permissible to include?</li> <li>Where can I download updated models of InstructLab?</li> <li>I have a question about the project. Where should I go?</li> <li>What are the software and hardware requirements for using InstructLab?</li> <li>Glossary</li> <li>Additional Resources</li> </ul>"},{"location":"community/FAQ/#document-summary","title":"Document summary","text":"<p>This page serves as a comprehensive FAQ for the InstructLab project, detailing how it works, how to begin contribution, and the goals behind the project. Key information includes:</p> <ul> <li>InstructLab Overview: This open source project allows users to interact with and train the Merlinite-7b (default) or Granite-7b AI Large Language Models (LLMs) by contributing skills and knowledge.</li> <li>LAB Method: A synthetic data-based tuning method for LLMs consisting of a taxonomy-driven data curation process, a synthetic data generator, and two-phased training with replay buffers.</li> <li>Contribution Process: Contributors can add skills or knowledge to the LLM by creating YAML files and testing changes locally before submitting a pull request to InstructLab\u2019s GitHub repository.</li> <li>Project Goals: To democratize contributions to AI and LLMs, allowing rapid model development through community collaboration facilitated by weekly builds that integrate community contributions.</li> </ul>"},{"location":"community/FAQ/#documentation-disclaimer","title":"Documentation disclaimer","text":"<p>There are currently three repositories that contain documentation crucial to getting users starting with the project:</p> <ul> <li>Community This repository shares InstructLab's activity and collaboration details across the community and include the most current information about the project. It should be approached as the primary repository for getting started, and contains procedures and links to relevant information to make the process as simple as possible.</li> <li><code>ilab</code> command-line interface (CLI) tool. This repository is responsible for the <code>ilab</code> CLI tool. It provides information about how to download the <code>ilab</code> CLI, how to contribute to the <code>ilab</code> CLI tool, among others.</li> <li>Taxonomy Tree. This repository is responsible for the taxonomy tree that allows you to create models tuned with your data. It provides information about what skills and knowledge are, how to create a pull request to contribute to the AI model, and expectations for pull request review.</li> </ul> <p>As this project grows, documentation and its organization will change. Members of this project will be made aware of significant changes and updates made to documentation.</p> <p>Unless otherwise noted, all documentation for the InstructLab project is licensed under the CC-BY-4.0 license.</p>"},{"location":"community/FAQ/#general-faq","title":"General FAQ","text":""},{"location":"community/FAQ/#what-is-instructlab","title":"What is InstructLab?","text":"<p>InstructLab (**L**arge-scale **A**lignment for chat**B**ots) is an open source initiative that provides a platform for easy engagement with AI Large Language Models (LLM) by using the <code>ilab</code> command-line interface (CLI) tool. You can use the CLI to work with Merlinite-7b or Granite-7b to test new skills and knowledge, for example, asking it to write a poem or answer a question about a particular subject. Users can then augment the LLM\u2019s capabilities by submitting the skills and knowledge they have tested to the project\u2019s taxonomy repository on GitHub by creating a pull request. This approach encourages community-driven enhancements without the need for complex model forking or fine-tuning of the model, promoting rapid development through collaborative contributions.</p>"},{"location":"community/FAQ/#what-is-lab","title":"What is LAB?","text":"<p>LAB (**L**arge-scale **A**lignment for chat**B**ots) is a novel synthetic data-based align tuning method for LLMs from IBM Research. It consists of three components:</p> <ol> <li>A taxonomy-drive data curation process</li> <li>A large-scale synthetic data generator</li> <li>Multi-phased-training with replay buffers</li> </ol> <p>The LAB approach allows incrementally adding new knowledge and skills to an already pre-trained model without catastrophic forgetting.</p> <p>More information about the LAB method can be found on the Hugging Face project page.</p>"},{"location":"community/FAQ/#how-does-instructlab-work","title":"How does InstructLab work?","text":"<p>InstructLab is driven by taxonomies and works by empowering users to add new skills and knowledge to a pre-trained LLM.</p>"},{"location":"community/FAQ/#what-are-the-goals-of-the-instructlab-project","title":"What are the goals of the InstructLab project?","text":"<p>In its current state, openly contributing to a large language model (LLM) has been difficult because of the large compute infrastructure needed to run one.</p> <p>The InstructLab project seeks to democratize the contribution to AI and LLMs through its taxonomy repository. When users contribute to the InstructLab project, the taxonomy repository resynthesizes the open source training data for InstructLab-trained LLMs. The model is then re-trained regularly (weekly builds), ensuring that community contributions are integrated while enriching the model\u2019s capabilities over time.</p>"},{"location":"community/FAQ/#how-can-i-contribute","title":"How can I contribute?","text":"<p>You can begin your contribution journey by reading over the Contributing guide and joining the Community Slack Channel.</p> <p>When you're ready to start contributing, you can follow the Getting Started guide. This guide shows you how to</p> <ul> <li>Install the <code>ilab</code> CLI.</li> <li>Deploy the LLM locally.</li> <li>Add skills or knowledge and train to the local LLM with your data.</li> <li>Create a pull request and add your information to the InstructLab taxonomy.</li> <li>Get reviews on your pull requests</li> </ul>"},{"location":"community/FAQ/#im-having-problems-with-the-ilab-cli-tool-what-should-i-do","title":"I'm having problems with the <code>ilab</code> CLI tool. What should I do?","text":"<p>A list of common problems associated with downloading the <code>ilab</code> CLI tool can be found in the CLI repository's discussion board.</p>"},{"location":"community/FAQ/#why-should-i-contribute","title":"Why should I contribute?","text":"<p>InstructLab is designed to enable collaboration around Merlinite-7b and Granite-7b, an open source licensed LLM that contributors can access through Hugging Face. Participating is an opportunity to contribute to open source AI regardless of technical background.</p> <p>When contributors write an addition to the existing taxonomy, make a pull request, and get it reviewed and merged, their changes are rolled out in the next build. This update strategy expedites the model\u2019s capabilities and allows contributors to see the impact that they have made on the model much sooner than other LLMs.</p>"},{"location":"community/FAQ/#what-large-language-models-llms-am-i-contributing-to-through-the-instructlab-project","title":"What large language models (LLMs) am I contributing to through the InstructLab project?","text":"<p>Contributions to the InstructLab project include fine-tuning Merlinite-7b or Granite-7b, an open-source licensed LLM. Contributors have direct access to the model they are improving through Hugging Face.</p>"},{"location":"community/FAQ/#what-is-merlinite-7b","title":"What is Merlinite-7b?","text":"<p>Merlinite-7b is a Mistral-7b derivative model fine-tuned with the LAB (**L**arge-scale **A**lignment for chat**B**ots) method using Mixtral-8x7b-Instruct as a teacher model.</p> <p>More information about the Merlinite-7b can be found on the Hugging Face project page.</p>"},{"location":"community/FAQ/#what-is-granite-7-lab","title":"What is Granite-7-lab?","text":"<p>Granite-7b-lab is a model that was built from scratch by IBM and fine tuned with the LAB (**L**arge-scale **A**lignment for chat**B**ots) method.</p> <p>More information about the Granite-7b can be found on the Hugging Face project page.</p>"},{"location":"community/FAQ/#what-is-a-skill","title":"What is a \u201cskill\u201d?","text":"<p>In the context of InstructLab, a skill is a capability domain submitted by a contributor intending to train the AI model on the submitted information. In other words, when you submit a skill, you teach the AI model how to do something.</p> <p>InstructLab skills are broken down into two main categories:</p> <ul> <li>Composition skills. Composition or performative skills allow AI models to perform specific tasks or functions. With InstructLab, there are two types of composition skills:</li> <li>Freeform compositional skills are performative skills that do not require additional context. For example, to train an AI model to write a poem, you would provide examples of poems.</li> <li>Grounded compositional skills are performative skills that require additional context. One example is how an AI model reads the value of a cell in a table layout. To create the grounded skill to read a table formatted in Markdown, the additional context might be an example table layout.</li> <li>Foundational skills. Foundational skills are skills like math, reasoning, and coding. Note: Foundational skills are not currently being accepted.</li> </ul> <p>Skills are written in a YAML file and submitted to the InstructLab upstream project for review. See the Skills: YAML examples for different types of examples.</p>"},{"location":"community/FAQ/#what-is-knowledge","title":"What is \u201cknowledge\u201d?","text":"<p>Knowledge consists of data and facts. When creating knowledge for an AI model, you are providing it with additional data and information to answer questions more accurately. Whereas skills are the information that trains an AI model on how to do something, knowledge is based on the AI model\u2019s ability to answer questions that involve facts, data, or references.</p> <p>Like skills, knowledge submissions are submitted in YAML format to the InstructLab upstream project for review. See the Knowledge: YAML examples for different types of examples.</p>"},{"location":"community/FAQ/#is-the-project-looking-for-certain-types-of-skill-contributions","title":"Is the project looking for certain types of skill contributions?","text":"<p>Currently, InstructLab only accepts compositional (freeform and grounded) skills and knowledge. However, any type of freeform or grounded skill can be submitted. Some skills might not be added to the taxonomy repository for reasons such as duplication, submitting a skill that the model already does well, or submitting a controversial skill.</p> <p>Foundational skills are not currently being accepted.</p> <p>For a list of accepted skills, see Accepted Skills.</p>"},{"location":"community/FAQ/#what-are-the-acceptance-criteria-for-a-skills-submission","title":"What are the acceptance criteria for a skills submission?","text":"<p>Skills should seek to add capabilities or a knowledge domain to the AI model; in other words, a skills submission should teach the AI model how to do something instead of providing information about something. A good skills submission might address something that the AI model does poorly and seek to enhance its ability to execute that capability better. For a list of commonly accepted skills, see Accepted Skills.</p> <p>Skills submissions that are unlikely to be accepted include submitting a knowledge request instead of a skills request, submitting a skill that the model already does well, submitting a controversial skill, or submitting skills that do not execute pure math or coding. For a list of skills to avoid submitting, see Skills to Avoid.</p>"},{"location":"community/FAQ/#what-are-the-acceptance-criteria-for-a-knowledge-submission","title":"What are the acceptance criteria for a knowledge submission?","text":"<p>Requirements for knowledge submissions can be found in the Getting Started with Knowledge Contributions guide.</p>"},{"location":"community/FAQ/#how-can-i-submit-a-skill-or-knowledge","title":"How can I submit a skill or knowledge?","text":"<p>For information about submitting a skill after you have identified a gap, see the Ways to contribute guide.</p>"},{"location":"community/FAQ/#what-happens-after-you-submit-a-pull-request","title":"What happens after you submit a pull request?","text":"<p>After a pull request is submitted, a review is conducted by both the Taxonomy Triage team and the Taxonomy Approvers team to ensure that they are relevant, actionable, and have all of the required information needed to be a valuable addition to the AI model. Triagers might provide feedback and use labels to manage the state of the submitted pull request. Triagers also might provide informative feedback and helpful comments to improve the submission. After the pull request is approved, a Taxonomy Approver merges the skill.</p> <p>More information regarding basic review questions, subjective review questions, labels, and the reasons for approval, further review requirements, or rejection can be found on the Triaging contributions page of the GitHub repository.</p>"},{"location":"community/FAQ/#how-are-submissions-reviewed","title":"How are submissions reviewed?","text":"<p>For code review, the project maintainers use LGTM (Looks Good to Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers.</p> <p>For skills and knowledge PRs, your PR will be checked to ensure it is relevant, actionable, and has all the information necessary for the approval team to review and merge the PR. The Triage team will use labels to manage the state and action of PRs as well as provide feedback to contributors based upon the following review guidelines:</p> <ul> <li>Does the PR have the pull request template information filled out?</li> <li>Did all the PR checks pass?</li> <li>Does the skill have three or more examples?</li> <li>Are the YAML fields correct?</li> <li>No PII in content</li> <li>Does this content include anything documented in the project's Avoid these Topics guidelines?</li> <li>Does it adhere to the Code of Conduct guidelines?</li> <li>Was a response clearly generated by the LLM?</li> </ul>"},{"location":"community/FAQ/#how-long-will-it-take-for-my-pull-request-to-be-reviewed","title":"How long will it take for my pull request to be reviewed?","text":"<p>Due to the large number of contributions currently being received, it is difficult to provide an exact timeline for reviewing your pull request.</p>"},{"location":"community/FAQ/#if-my-pull-request-is-accepted-how-long-will-it-take-for-my-changes-to-appear-in-the-next-model-update","title":"If my pull request is accepted, how long will it take for my changes to appear in the next model update?","text":"<p>After a pull request is accepted, the changes are regularly incorporated into InstructLab.</p>"},{"location":"community/FAQ/#what-is-the-software-license-for-instructlab","title":"What is the software license for InstructLab?","text":"<p>The InstructLab project as well as the Merlinite-7b and Granite-7b models are distributed under Apache License, Version 2.0.</p>"},{"location":"community/FAQ/#what-is-the-content-license-for-instructlab-documentation","title":"What is the content license for InstructLab documentation?","text":"<p>Unless otherwise specified, all documentation for InstructLab is licensed under the CC-BY-4.0 license from Creative Commons.</p>"},{"location":"community/FAQ/#am-i-required-to-license-code-submissions-to-instructlab-under-the-apache-20-license","title":"Am I required to license code submissions to InstructLab under the Apache 2.0 license?","text":"<p>Yes. Code contributions to the InstructLab project are subject to the terms and conditions under the Apache 2.0 license.</p>"},{"location":"community/FAQ/#my-contribution-requires-submitting-data-along-with-code-what-data-is-permissible-to-include","title":"My contribution requires submitting data along with code. What data is permissible to include?","text":"<p>It is recommended that third-party content be licensed with an open data license that does not restrict commercial use or the creation of derivative works, including the following licenses:</p> <ul> <li>CC0</li> <li>CDLA-Permissive</li> <li>CC-BY-4.0</li> <li>CC-BY-4.0 SA</li> <li>Apache 2.0</li> <li>MIT</li> </ul>"},{"location":"community/FAQ/#do-submissions-to-the-project-require-a-contributor-license-agreement-of-some-kind","title":"Do submissions to the project require a contributor license agreement of some kind?","text":"<p>The InstructLab project follows the same approach (the Developer's Certificate of Origin 1.1 (DCO)) that the Linux Kernel community uses to manage code contributions. Unless the file says otherwise for this project, the relevant open source license is the Apache License, Version 2.0.  When submitting a patch for review, you must include a sign-off statement in the commit message. See the \"Legal\" section of the Contributing document.</p> <p>You can find more information about useful tools for managing DCO sign-off in our Community Contributions Guide.</p>"},{"location":"community/FAQ/#where-can-i-download-updated-models-of-instructlab","title":"Where can I download updated models of InstructLab?","text":"<p>The latest version of InstructLab can be downloaded using the <code>ilab download</code> CLI command.</p>"},{"location":"community/FAQ/#i-have-a-question-about-the-project-where-should-i-go","title":"I have a question about the project. Where should I go?","text":"<p>Currently, the best method for communicating with peers and project maintainers is in the Community Slack Channel. Visit our InstructLab Slack Workspace Guide for information on how to join.</p> <p>TODO: Update with mailing list details once these are created. Related issue https://github.com/instructlab/community/issues/89</p>"},{"location":"community/FAQ/#what-are-the-software-and-hardware-requirements-for-using-instructlab","title":"What are the software and hardware requirements for using InstructLab?","text":"<p>The local training is the most hardware intensive part of this process. Your hardware determines how fast/slow training the model locally will take.</p> <p>To run and train InstructLab locally, you must meet the following requirements:</p> <ul> <li>A supported operating system</li> <li>A Linux-based operating system</li> <li>An Apple Silicon M1, M2, or M3 system</li> <li>A Windows system with WSL (Windows Subsystem for Linux)</li> <li>Python 3.9 or later, including the development headers</li> <li>Approximately 10GB of free disk space to get through the <code>ilab generate</code> step</li> <li>Approximately 60GB of free disk space is needed to run the entire process locally on Apple hardware</li> <li>About 32 GB RAM</li> </ul>"},{"location":"community/FAQ/#glossary","title":"Glossary","text":"Term Explanation Additional Reference Checkpoints Snapshots during training. They are scored individually and the best is selected. N/A CUDA \u201cCompute Unified Device Architecture\u201d - A parallel computing platform and API for general computing on GPUs by NVIDIA. Ref DeepSpeed Deep learning optimization library for PyTorch Ref Granite Open source licensed LLM released by IBM Ref FSDP \u201cFull Sharded Data Parallel\u201d - A wrapper for sharding module parameters across data parallel workers, used within PyTorch Ref LAB \u201cLarge-Scale Alignment for ChatBots\u201d Ref Labradorite LAB-enhanced Llama2 model N/A Llama LLM released by Meta N/A Llama CPP A C++ library for inference of Llama models, similar to vLLM Ref LoRA \u201cLow Rank Adapter\u201d - Fine-tuning algorithm used within PyTorch Ref Merlinite LAB-enhanced Mistral model developed by IBM N/A Mistral LLM released by Mistral AI N/A Mixtral LLM using Mixture of Experts by Mistral AI N/A MMLU \u201cMassive Multitask Language Understanding\u201d - An evaluation scheme used for knowledge benchmarking Ref MLX An array framework for machine learning research on Apple Silicon chips Ref MPS \u201cMetal Performance Shaders\u201d - A MacOS hardware accelerator, similar to CUDA kernels N/A MT-Bench \u201cMulti-turn benchmark\u201d - An evaluation scheme used for skills benchmarking Ref PEFT \u201cParameter Efficient Fine-Tuning\u201d N/A PR-bench Evaluation scheme used for skills PR benchmarking N/A PR-mmlu Evaluation scheme used for knowledge PR benchmarking N/A PyTorch Library supporting tensors and dynamic neural networks in Python with strong GPU acceleration Ref QLoRA \"\u201cQuantized Low Rank Adapter\u201d - Fine tuning algorithm used within PyTorch Ref Quantization Process of reducing resource needs for a model by decreasing the range of the data type Ref SDG \u201cSynthetic Data Generation\u201d - The process where a model artificially generates data based on provided examples. N/A vLLM A library for LLM inference and serving, similar to Llama CPP. Provides an OpenAI-compatible API. Ref"},{"location":"community/FAQ/#additional-resources","title":"Additional Resources","text":"<p>Additional resources, including the Code of Conduct, Code of Conduct Committee members, how to contribute, how to join the Slack channel, and more, can be found in the following repositories:</p> <p>InstructLab Taxonomy Repository</p> <p>InstructLab CLI Repository</p> <p>InstructLab Community Repository</p> <p>Slack and communication</p> <ul> <li>Joining the Slack Channel</li> <li>Slack Moderation</li> </ul>"},{"location":"community/GOVERNANCE/","title":"InstructLab Governance","text":"<p>The following document outlines how the InstructLab project governance operates.</p>"},{"location":"community/GOVERNANCE/#the-instructlab-project","title":"The InstructLab Project","text":"<p>InstructLab is made up of several projects that are defined as codebases and services with different release cycles. Collectively, these enable large-model development. Currently, these projects include the following:</p> <ul> <li><code>ilab</code> command-line interface (CLI) tool. This repository is responsible for the <code>ilab</code> command-line interface (CLI) tool.</li> <li>taxonomy tree. This repository is responsible for the taxonomy tree that allows you to create models tuned with your data.</li> </ul>"},{"location":"community/GOVERNANCE/#governance-structure-and-roadmap","title":"Governance Structure and Roadmap","text":"<p>The InstructLab Project has a two-level governance structure with an Oversight Committee and Project Maintainers.</p> <p>Except where otherwise noted, decisions should always start at the most local level of project governance. For example, decisions that affect only one project, such as the taxonomy repository and not the <code>ilab</code> CLI tool, can happen within the taxonomy project. While communication between the different project teams is important as they are all interconnected, minor decisions do not need organization-wide consensus and can be moved forward at the project level.</p> <p>Changes in maintainership and other governance are currently announced on the InstructLab community Slack channel. Directions to join the Slack channel can be found here. Changes are also announced to the announce mailing list.</p>"},{"location":"community/GOVERNANCE/#project-maintainers-overview","title":"Project Maintainers overview","text":"<p>Project Maintainers focus on a single codebase, a group of related codebases, a service (for example, a website), or a project to support other projects (such as marketing or community management).</p> <p>Project Maintainers are responsible for activities surrounding the development and release of code, the operation of any services that they own, or the tasks needed to execute their project (for example, community management or setting up an event booth). Technical decisions for code reside with the project Maintainers unless there is a decision related to multiple maintainer groups that cannot be resolved by those groups. Those cases can be escalated to the Oversight Committee.</p> <p>To be considered an active project Maintainer, it is required to be associated with at least one active, non-archived project. If only listed on archived projects, they become emeritus Maintainers and are no longer eligible to become an organization Maintainer.</p> <p>Project Maintainers do not need to be software developers; however, they must be substantial contributors. For example, if a repository is for documentation it would be appropriate for a project Maintainer to be an editor or technical writer.</p> <p>Advancement to the project Maintainer position, removal or stepping down, and duties are detailed in the Contributor Roles. The list of current maintainers can be found here.</p>"},{"location":"community/GOVERNANCE/#instructlab-oversight-committee-overview","title":"InstructLab Oversight Committee Overview","text":"<p>The initial Oversight Committee at the launch of the project was appointed by the founding sponsors of the InstructLab project. This bootstrap committee will serve until the first election of the Oversight Committee using processes and timing as determined by this group.</p> <p>The list of Oversight Committee members can be found in MAINTAINERS.md.</p> <p>The Oversight Committee consists of 3 to 7 leaders on the InstructLab project.  These members will serve to supervise the overall project and its health. It will also consist of a selected Chair member who will set agendas and call meetings. These meetings can be public or private at the discretion of the Oversight Committee.</p> <p>The Oversight Committee is responsible for the following duties:</p> <ul> <li>Maintaining the mission, vision, values, and scope of the project</li> <li>Refining the governance and charter as needed</li> <li>Making project-level decisions, including setting technical policies that apply across all components</li> <li>Resolving escalated project decisions when the team responsible is blocked</li> <li>Managing the InstructLab brand</li> <li>Controlling access to InstructLab assets such as source repositories and hosting</li> <li>Appointing members to the Code of Conduct Committee</li> <li>Deciding what projects are part of the InstructLab project</li> <li>Overseeing the resolution and disclosure of security issues</li> <li>Managing financial decisions related to the project</li> </ul>"},{"location":"community/GOVERNANCE/#draft-oversight-committee-selection-process","title":"Draft Oversight Committee selection process","text":"<p>Note</p> <p>This section is a draft. It is a responsibility of the initial Oversight Committee to finalize this process.</p> <p>The Oversight Committee will be selected and maintained using the following process:</p> <p>A project Maintainer of any active (non-archived) InstructLab organization project is eligible for a position as an organization Maintainer. Once a year, the Oversight Committee will be re-elected. The election will consist of a nomination period followed by an election period. Any person who has made a contribution to any repository under the InstructLab GitHub organization may nominate a suitable project Maintainer of an active project.</p> <p>The election will proceed according to the following process:</p> <ol> <li> <p>The nomination period will be three weeks. This period starts from the day after an organization Maintainer opening becomes available.</p> </li> <li> <p>The nomination must be made on the InstructLab community mailing list.</p> </li> <li> <p>After a nominated individual agrees to be a candidate for the Oversight Committee, project Maintainers will vote. The voting period will be open for a minimum of three business days. It will remain open until a supermajority of project Maintainers have voted. Only current Maintainers of active projects are eligible to vote.</p> </li> <li> <p>When the number of nominated individuals matches the number of openings, each individual must have a Yes vote from a supermajority of those who voted.</p> </li> <li> <p>When there are more individuals than open positions, voting will use a Ranked Choice voting method, such as Condorcet.</p> </li> </ol>"},{"location":"community/GOVERNANCE/#resignation-or-departure-from-the-maintainer-or-the-oversight-committee-role","title":"Resignation or Departure from the Maintainer or the Oversight Committee role","text":"<p>Project Maintainers or Oversight Committee members may resign or could be expelled as follows:</p> <ul> <li> <p>Maintainers or an Oversight Committee member may step down through email. Within 7 calendar days, organization contributors and Maintainers will be notified on the InstructLab community mailing list.</p> </li> <li> <p>After an Oversight Committee member steps down, they become an emeritus Maintainer.</p> </li> <li> <p>Maintainers and Committee members MUST remain active on the project. In the event that an Oversight Committee member or a Maintainer is unresponsive or inactive for more than 3 months, they may be removed by a supermajority vote.</p> </li> <li> <p>Maintainers and Oversight Committee members who have violated the Code of Conduct may be removed by a supermajority vote of the remaining Oversight Committee members.</p> </li> </ul>"},{"location":"community/GOVERNANCE/#decision-making-at-the-instructlab-organization-level","title":"Decision making at the InstructLab organization level","text":"<p>Generally, there are methods for decision making for the InstructLab project: by lazy consensus or by voting.</p> <p>The default decision-making process is lazy-consensus. This means that any decision is considered supported by the team making it so long as no one objects. Silence on any consensus decision is implicit agreement, and equivalent to explicit agreement. An explicit agreement may be stated at will.</p> <p>When a consensus cannot be met, a Maintainer can call for a majority vote on a decision.</p> <p>Many of the day-to-day project maintenance can be done through the lazy consensus model.</p> <p>The secondary decision-making process is done by voting. The following items are examples that must be called to a vote and conducted by the appropriate body:</p> <ul> <li>Appointing or removing a member of the Code of Conduct Committee (supermajority of the Oversight Committee)</li> <li>Carrying out Code of Conduct decisions requiring severe censure (majority of the Code of Conduct committee)</li> <li>Removing a Maintainer for any reason other than inactivity (supermajority of the Oversight Committee)</li> <li>Non-trivial changes to the governance (this document) (supermajority of the Oversight Committee)</li> <li>Licensing and intellectual property changes such as new logos or wordmarks (majority of the Oversight Committee)</li> <li>Adding, archiving, or removing projects (majority of the Oversight Committee)</li> </ul> <p>Other decisions may be called out and put up for decision on the InstructLab community mailing list. This can be done by anyone at any time. By default, any decisions called to a vote will be for a simple majority vote of the Oversight Committee.</p>"},{"location":"community/GOVERNANCE/#code-of-conduct","title":"Code of Conduct","text":"<p>InstructLab's Code of Conduct is enforced by the Code of Conduct Committee (CoCC). This committee will be appointed and removed by the Oversight Committee using a supermajority vote.</p> <p>The CoCC is responsible for investigating, evaluating, and recommending remedies for substantiated Code of Conduct incidents to the appropriate body. The CoCC will judge possible violations around principles of restorative justice rather than punishment. All teams within InstructLab are obligated to support the CoCC's recommendations on remedies.</p> <p>Current CoCC members can be found on the Code of Conduct Committee page.</p> <p>Possible Code of Conduct violations should be reported to the Code of Conduct Committee via the Code of Conduct email alias.</p>"},{"location":"community/GOVERNANCE/#developer-certificate-of-origin-dco-and-licenses","title":"Developer Certificate of Origin (DCO) and Licenses","text":"<p>The following licenses and contributor agreements will be used for InstructLab projects:</p> <ul> <li>Apache 2.0 for code</li> <li>Creative Commons Attribution 4.0 International Public License for documentation</li> <li>Developer Certificate of Origin for new contributions</li> </ul>"},{"location":"community/GOVERNANCE/#modifications-to-this-governance","title":"Modifications to this Governance","text":"<p>This governance may be modified by a supermajority vote of the Oversight Committee.</p> <p>Trivial changes that do not introduce policy changes may be approved by two members of the Oversight Committee.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/","title":"InstructLab Slack Workspace Guide","text":"<p>The purpose of this document is to inform folks about how to join the InstructLab Slack Workspace and document the channels therein. We look forward to meeting everyone and welcoming you on Slack!</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#overview","title":"Overview","text":"<p>The InstructLab Slack workspace resides at https://instruct-lab.slack.com. You must join via this invitation link</p> <p>Upon joining, you will automatically be added to our <code>#announce</code> channel. You are welcome and encouraged to join other channels.</p> <p>All discussions in the InstructLab Slack are governed by our project code of conduct.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#channel-overview","title":"Channel Overview","text":"<ul> <li><code>#dev</code> Cross-project coordination discussion, such as topics that cover both frontend and backend development for InstructLab</li> <li><code>#admin</code> Place to get non-technical help. If you don't know where to go after reading this guide, join this channel for air traffic control.</li> <li><code>#announce</code> Project wide announcements such as releases, reminders about community calls, and celebrating new maintainers. This channel is moderated (only Workspace Administrators can post) and low-traffic.</li> <li><code>#backend</code> Backend work for the InstructLab project, including pipeline for synthetic data generation, training, model evaluation, and publishing.</li> <li><code>#community</code> Place to discuss community matters such as improving the contributor experience, getting help reviewing a presentation about InstructLab you want to give at a meetup, or learning how you can contribute to InstructLab beyond software development.</li> <li><code>#contribhelp</code> General questions about getting started as an InstructLab contributor. This channel is the place to go if you need help with your first pull request.</li> <li><code>#docs</code> Documentation team discussions and questions about documentation.</li> <li><code>#infra</code> Topics related to project infrastructure, such as repo maintenance, planned outages, or who has the keys to the social media accounts.</li> <li><code>#frontend</code> Frontend work for the InstructLab project, including the CLI tool and User Interface</li> <li><code>#social</code> Place to chat and enjoy camaraderie with fellow community members.</li> <li><code>#triage</code> Triage team discussions.</li> <li><code>#users</code> InstructLab users forum for troubleshooting and sharing tips and tricks.</li> <li><code>#github-bot</code> Place to discuss, brainstorm and hack code for InstructLab GitHub Bot.</li> </ul>"},{"location":"community/InstructLab_SLACK_GUIDE/#usings-threaded-replies-in-slack","title":"Usings Threaded Replies in Slack","text":"<p>By default, we use threaded messages in Slack so as to keep all responses to a particular topic grouped together. Please reply to specific messages by replying in thread.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#moderation-and-reporting-abuse","title":"Moderation and Reporting Abuse","text":"<p>We are an open, welcoming, and inclusive community and expect our members to be kind and respectful in all discourse.</p> <p>We take reports of harassment very seriously and will action any reports of inappropriate behavior as quickly as possible.</p> <p>To learn how to report abuse - and to whom you will be reporting - please see our InstructLab slack Moderation Guide.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#having-trouble-joining","title":"Having Trouble Joining?","text":"<p>If you are having trouble joining the InstructLab Slack, please file an issue in the community repo so we can help you.</p> <p>TODO: Update with email address to get help once these are set up.</p>"},{"location":"community/InstructLab_SLACK_GUIDE/#private-channels","title":"Private Channels","text":"<p>InstructLab is an open source project and we value defaulting to open in all of our community communications. There are some cases where discussions must happen in private. For the sake of transparency, we are documenting these private channels and what they are used for.</p> <ul> <li><code>#code-of-conduct-committee</code> Space for the InstructLab Code of Conduct Committee to discuss any reports of harassment or other violations of the project Code of Conduct and how to respond to them.</li> <li><code>#mods</code> Space for the InstructLab Workspace Administrators to confer privately only when necessary. We default to open and hold each other accountable to do so.</li> </ul>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/","title":"InstructLab Slack Moderation Guide","text":"<p>The purpose of this document is both describe how users of the InstructLab's Slack workspace can report abuse in the Slack workspace and to provide space administrators with an easy to use how to guide for channel moderation.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#reporting-abuse","title":"Reporting Abuse","text":"<p>Should any community members using the InstructLab Slack workspace feel that they have experienced behavior that violates our project Code of Conduct, they are welcome and encouraged to contact the members of the Code of Conduct Committee for help. Mentioning <code>@cocc</code> will page all members of the committee so that they can assist you.</p> <p>In the event that you do not receive help within a timely fashion - and we will do our very best to respond right away - you can ask for help from the workspace admins by either joining channel <code>#admin</code> or mentioning <code>@admins</code>.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#moderation-guide","title":"Moderation Guide","text":"<p>Moderation activities can only be performed by users who are designated as workspace administrators.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#workspace-administrators","title":"Workspace Administrators","text":"<p>At time of writing, our workspace administrators/moderators are as follows:</p> <ul> <li>Aakanksha Duggal</li> <li>Ali Maredia</li> <li>Alina Ryan</li> <li>Cara Delia +</li> <li>Carol Chen +</li> <li>Charlie Doern</li> <li>Dan McPherson</li> <li>Jaideep Rao</li> <li>James Kunstle</li> <li>Jason Greene</li> <li>Jeremy Eder</li> <li>JJ Asghar +</li> <li>Joe Sepi +</li> <li>Kelly Brown</li> <li>Leslie Hawthorn +</li> <li>Mo McElaney +</li> <li>M\u00e1ir\u00edn Duffy</li> <li>Nathan Weinberg</li> <li>Oleg Silkin</li> <li>Russell Bryant</li> </ul> <p>+ Members of the Code of Conduct Committee</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#how-we-moderate","title":"How We Moderate","text":""},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#deleting-inappropriate-comments","title":"Deleting Inappropriate Comments","text":"<p>Upon report of abuse to the Code of Conduct Committee or, alternatively if needed to the workspace administrators due to a coverage gap, the appropriate parties will assess the situation.</p> <p>The first step will be to remind folks to abide by the project Code of Conduct.</p> <p>Inappropriate or offensive messages will be deleted.</p> <p>Deleting a message shall be done at the sole discretion of the Code of Conduct Committee and/or workspace administrators.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#how-to-delete-a-message","title":"How to delete a message","text":""},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#removing-workspace-members","title":"Removing Workspace Members","text":"<p>Admins should consider first removing the offending person from the channel in which the unacceptable behavior occurred and having a conversation with them as a DM to remind them of their responsibilities to abide by the project code of conduct as part of their participation in the InstructLab community.</p> <p>If a user is a repeat offender, after being warned, their account can be deactivated by an admin.</p>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#removing-someone-from-a-channel","title":"Removing someone from a channel","text":"<ul> <li>By default, Workspace Owners and Admins can remove people from public channels, and members can remove people from private channels.</li> <li>Anyone can be removed from a channel by those with permission.</li> <li>All members and guests need to be added back to a private channel to rejoin it, and guests also need to be added back to a public channel to rejoin it.</li> <li>It's not possible to remove people from the #announce channel. However, posting in this channel is restricted to workspace administrators by default.</li> </ul>"},{"location":"community/InstructLab_SLACK_MODERATION_GUIDE/#deactivating-a-members-account","title":"Deactivating a member's account","text":""},{"location":"getting-started/creating_new_knowledge_or_skills/","title":"Creating New Knowledge or Skills","text":""},{"location":"getting-started/creating_new_knowledge_or_skills/#creating-new-knowledge-or-skills-and-training-the-model","title":"\ud83d\udcbb Creating new knowledge or skills and training the model","text":""},{"location":"getting-started/creating_new_knowledge_or_skills/#contribute-knowledge-or-compositional-skills","title":"\ud83c\udf81 Contribute knowledge or compositional skills","text":"<p>Detailed contribution instructions can be found in the taxonomy repository.</p> <p>Important</p> <p>There is a limit to how much content can exist in the question/answer pairs for the model to process. Due to this, only add a maximum of around 2300 words to your question and answer seed example pairs in the <code>qna.yaml</code> file.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#list-and-validate-your-new-data","title":"\ud83d\udcdc List and validate your new data","text":"<p>You can use the <code>ilab taxonomy diff</code> command to ensure <code>ilab</code> is registering your new knowledge or skills and your contributions are properly formatted. This command displays any new or modified YAML files within your taxonomy tree. For example, the following is the expected result of a valid compositional skill contribution after adding a new skill called <code>foo-lang</code> to the freeform writing subdirectory:</p> <pre><code>(venv) $ ilab taxonomy diff\ncompositional_skills/writing/freeform/foo-lang/qna.yaml\nTaxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)\n</code></pre> <p>You can also validate your entire taxonomy by performing a diff against an empty base by using the <code>--taxonomy-base=empty</code> argument:</p> <pre><code>(venv) $ ilab taxonomy diff --taxonomy-base=empty\ncompositional_skills/general/tables/empty/qna.yaml\ncompositional_skills/general/tables/editing/add_remove/qna.yaml\n...\nTaxonomy in $HOME/.local/share/instructlab/taxonomy is valid :)\n</code></pre>"},{"location":"getting-started/creating_new_knowledge_or_skills/#generate-a-synthetic-dataset","title":"\ud83d\ude80 Generate a synthetic dataset","text":"<p>Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, <code>ilab data generate</code> can start a server for you if you provide a fully qualified model path via <code>--model</code>.</p> <p>1) To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:</p> <p>With GPU acceleration:</p> <pre><code>ilab data generate --pipeline full --gpus &lt;NUM_OF_GPUS&gt;\n</code></pre> <p>Without GPU acceleration:</p> <pre><code>ilab data generate --pipeline simple\n</code></pre> <p>Use a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1) to generate data, run the following command:</p> <pre><code>ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n</code></pre> <p>Note</p> <p>\u23f3 This can take from 15 minutes to 1+ hours to complete, depending on your computing resources.</p> <p>Example output of <code>ilab data generate</code></p> <pre><code>(venv) $ ilab data generate\nINFO 2024-07-30 19:57:44,093 numexpr.utils:161: NumExpr defaulting to 8 threads.\nINFO 2024-07-30 19:57:44,452 datasets:58: PyTorch version 2.3.1 available.\nGenerating synthetic data using 'simple' pipeline, '$HOME/.cache/instructlab/models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf' model, './taxonomy' taxonomy, against http://localhost:8000/v1 server\nINFO 2024-07-30 19:57:45,084 instructlab.sdg:375: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.\nINFO 2024-07-30 19:57:45,090 instructlab.sdg.pipeline:153: Running pipeline single-threaded\nINFO 2024-07-30 19:57:47,820 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False\nINFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:197: Running block: gen_skill_freeform\nINFO 2024-07-30 19:57:47,820 instructlab.sdg.pipeline:198: Dataset({\n   features: ['task_description', 'seed_question', 'seed_response'],\n   num_rows: 5\n})\nINFO 2024-07-30 20:02:16,455 instructlab.sdg:411: Generated 1 samples\n...\n</code></pre> <p>The synthetic data set will be two files in the newly created in the datasets directory. On Linux this will be: <code>~/.local/share/instructlab/datasets</code> and on MacOS this will be <code>~/Library/Application Support/instructlab/datasets</code>. These files will be named <code>skills_train_msgs_*.jsonl</code> and <code>knowledge_train_msgs_*.jsonl</code>.</p> <p>2) Verify the files have been created by running the <code>ls datasets</code> command. Note: you must be in your <code>XDG_DATA_HOME/instructlab</code> directory.</p> <pre><code>(venv) $ ls datasets/\nnode_datasets_2024-08-12T20_31_15                          test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_23_06.jsonl\nknowledge_recipe_2024-08-12T20_31_15.yaml                      node_datasets_2024-08-13T19_51_48                          test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl\nknowledge_recipe_2024-08-13T19_51_48.yaml                      skills_recipe_2024-08-12T20_31_15.yaml                     test_mixtral-8x7b-instruct-v0-1_2024-08-13T19_47_59.jsonl\nknowledge_train_msgs_2024-08-12T20_31_15.jsonl                 skills_recipe_2024-08-13T19_51_48.yaml                     test_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl\nknowledge_train_msgs_2024-08-13T19_51_48.jsonl                 skills_train_msgs_2024-08-12T20_31_15.jsonl                train_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl\nmessages_mixtral-8x7b-instruct-v0-1_2024-08-12T20_31_15.jsonl  skills_train_msgs_2024-08-13T19_51_48.jsonl                train_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl\nmessages_mixtral-8x7b-instruct-v0-1_2024-08-13T19_51_48.jsonl  test_mixtral-8x7b-instruct-v0-1_2024-08-12T20_13_21.jsonl\n</code></pre> <p>Optional: It is also possible to run the generate step against a different model via an    OpenAI-compatible API. For example, the one spawned by <code>ilab model serve</code> or any remote or locally hosted LLM (e.g. via <code>ollama</code>, <code>LM Studio</code>, etc.). Run the following command:</p> <pre><code>ilab data generate --endpoint-url http://localhost:8000/v1\n</code></pre> <p>Note that it is also possible to generate a synthetic dataset based on the entire contents of the taxonomy repo using the <code>--taxonomy-base=empty</code> option:</p> <pre><code>ilab data generate --taxonomy-base=empty\n</code></pre>"},{"location":"getting-started/creating_new_knowledge_or_skills/#training-the-model","title":"\ud83d\udc69\ud83c\udfeb Training the model","text":"<p>There are many options for training the model with your synthetic data-enhanced dataset.</p> <p>Note</p> <p>Every <code>ilab</code> command needs to run from within your Python virtual environment.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#train-the-model-locally-on-linux","title":"Train the model locally on Linux","text":"<pre><code>ilab model train\n</code></pre> <p>Note</p> <p>\u23f3 This step can potentially take several hours to complete depending on your computing resources. Please stop <code>ilab model chat</code> and <code>ilab model serve</code> first to free resources.</p> <p>If you are using <code>ilab model train --legacy</code> or are on MacOS:</p> <p><code>ilab model train</code> outputs a brand-new model that can be served in the <code>models</code> directory called <code>ggml-model-f16.gguf</code>.</p> <p>If you are using <code>ilab model train</code> with a GPU enabled system:</p> <p><code>ilab model train</code> outputs brand-new models that can be served in the <code>~/.local/share/instructlab/checkpoints</code> directory.  These models can be run through <code>ilab model evaluate</code> to choose the best one.</p> <p>If you are using <code>ilab model train --strategy lab-multiphase</code></p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#train-the-model-locally-on-an-m-series-mac","title":"Train the model locally on an M-series Mac","text":"<p>To train the model locally on your M-Series Mac is as easy as running:</p> <pre><code>ilab model train\n</code></pre> <p>Note</p> <p>\u23f3 This process will take a little while to complete (time can vary based on hardware and output of <code>ilab data generate</code> but on the order of 5 to 15 minutes)</p> <p><code>ilab model train</code> outputs a brand-new model that is saved in the <code>&lt;model_name&gt;-mlx-q</code> directory called <code>adapters.npz</code> (in <code>Numpy</code> compressed array format). For example:</p> <pre><code>(venv) $ ls instructlab-merlinite-7b-lab-mlx-q\nadapters-010.npz        adapters-050.npz        adapters-090.npz        config.json             tokenizer.model\nadapters-020.npz        adapters-060.npz        adapters-100.npz        model.safetensors       tokenizer_config.json\nadapters-030.npz        adapters-070.npz        adapters.npz            special_tokens_map.json\nadapters-040.npz        adapters-080.npz        added_tokens.json       tokenizer.jso\n</code></pre>"},{"location":"getting-started/creating_new_knowledge_or_skills/#train-the-model-locally-with-gpu-acceleration","title":"Train the model locally with GPU acceleration","text":"<p>Training has experimental support for GPU acceleration with NVIDIA CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.</p> <pre><code>ilab model train --device=cuda\n</code></pre> <p>This version of <code>ilab model train</code> outputs brand-new models that can be served in the <code>~/.local/share/instructlab/checkpoints</code> directory on Linux and <code>~/Library/Application Support/instructlab/checkpoints</code> on MacOS.  These models can be run through <code>ilab model evaluate</code> to choose the best one.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#train-the-model-locally-with-multi-phase-training-and-gpu-acceleration","title":"Train the model locally with multi-phase training and GPU acceleration","text":"<p><code>ilab model train</code> supports multi-phase training. This results in the following workflow:</p> <ol> <li>We train the model on knowledge</li> <li>Evaluate the trained model to find the best checkpoint</li> <li>We train the model on skills</li> <li>We evaluate the model to find the best overall checkpoint</li> </ol> <pre><code>ilab model train --strategy lab-multiphase --phased-phase1-data &lt;knowledge train messages jsonl&gt; --phased-phase2-data &lt;skills train messages jsonl&gt; -y\n</code></pre> <p>This command takes in two <code>.jsonl</code> files from your <code>datasets</code> directory, one is the knowledge jsonl and the other is a skills jsonl. The <code>-y</code> flag skips an interactive prompt asking the user if they are sure they want to run multi-phase training.</p> <p>Note: this command may take 3 or more hours depending on the size of the data and number of training epochs you run.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#test-the-newly-trained-model","title":"\ud83d\udcdc Test the newly trained model","text":"<ul> <li>Run the following command to test the model:</li> </ul> <pre><code>ilab model test\n</code></pre> <p>The output from the command will consist of a series of outputs from the model before and after training.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#evaluate-the-newly-trained-model","title":"\ud83e\uddea Evaluate the newly trained model","text":"<p>You can use the <code>ilab model evaluate</code> command to evaluate the models you are training with several benchmarks. Currently, four benchmarks are supported.</p> Benchmark Measures Full Name Description Reference MMLU Knowledge Massive Multitask Language Understanding Tests a model against a standardized set of knowledge data and produces a score based on the model's performance Measuring Massive Multitask Language Understanding MMLUBranch Knowledge N/A Tests your knowledge contributions against a base model and produces a score based on the difference in performance N/A MTBench Skills Multi-turn Benchmark Tests a model's skill at applying its knowledge against a judge model and produces a score based on the model's performance MT-Bench (Multi-turn Benchmark) MTBenchBranch Skills N/A Tests your skill contributions against a judge model and produces a score based on the difference in performance N/A <p>Note</p> <p>MTBench and MTBenchBranch use prometheus-8x7b-v2.0 as the judge model by default. While you do not need to use this model as your judge, it is strongly recommended to do so if you have the necessary hardware resources. You can download it via <code>ilab model download</code>.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#running-mmlu","title":"Running MMLU","text":"<p>Below is an example of running MMLU on a local model with minimal tasks:</p> <pre><code>export INSTRUCTLAB_EVAL_MMLU_MIN_TASKS=true   # don't set this if you want to run full MMLU\nexport ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\nilab model evaluate --benchmark mmlu --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab\n...\n# KNOWLEDGE EVALUATION REPORT\n\n## MODEL\n/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab\n\n### AVERAGE:\n0.45 (across 3)\n\n### SCORES:\nmmlu_abstract_algebra - 0.35\nmmlu_anatomy - 0.44\nmmlu_astronomy - 0.55\n</code></pre> <p>Below is an example of running MMLU on a Hugging Face model with minimal tasks:</p> <pre><code>export INSTRUCTLAB_EVAL_MMLU_MIN_TASKS=true   # don't set this if you want to run full MMLU\nilab model evaluate --benchmark mmlu --model instructlab/granite-7b-lab\n...\n# KNOWLEDGE EVALUATION REPORT\n\n## MODEL\ninstructlab/granite-7b-lab\n\n### AVERAGE:\n0.45 (across 3)\n\n### SCORES:\nmmlu_abstract_algebra - 0.35\nmmlu_anatomy - 0.44\nmmlu_astronomy - 0.55\n</code></pre> <p>Note</p> <p>Currently, MMLU can only be run against a safetensors model directory, either locally or on Hugging Face. GGUFs are not currently supported.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#running-mmlubranch","title":"Running MMLUBranch","text":"<p>Below is an example of running MMLUBranch with a local safetensors model directory:</p> <pre><code>export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\nilab model evaluate --benchmark mmlu_branch --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab\n...\n# KNOWLEDGE EVALUATION REPORT\n\n## BASE MODEL\n/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab\n\n## MODEL\n/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab\n\n### AVERAGE:\n+0.0 (across 1)\n\n### NO CHANGE:\n1. tonsils\n</code></pre> <p>Below is an example of running MMLUBranch with Hugging Face models:</p> <pre><code>ilab model evaluate --benchmark mmlu_branch --model instructlab/granite-7b-lab --base-model instructlab/granite-7b-lab\n...\n# KNOWLEDGE EVALUATION REPORT\n\n## BASE MODEL\ninstructlab/granite-7b-lab\n\n## MODEL\ninstructlab/granite-7b-lab\n\n### AVERAGE:\n+0.0 (across 1)\n\n### NO CHANGE:\n1. tonsils\n</code></pre> <p>Tip</p> <p>You can mix and match running local models and remote models on Hugging Face, so long as a safetensors model is present.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#running-mtbench","title":"Running MTBench","text":"<p>Below is an example of running MTBench with a local safetensors model directory:</p> <pre><code>export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\nilab model evaluate --benchmark mt_bench --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab --judge-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab\n...\n# SKILL EVALUATION REPORT\n\n## MODEL\n/home/example-user/.local/share/instructlab/models/instructlab/granite-7b-lab\n\n### AVERAGE:\n8.07 (across 91)\n\n### TURN ONE:\n8.64\n\n### TURN TWO:\n7.19\n\n### ERROR RATE:\n0.43\n</code></pre> <p>Below is an example of running MTBench with local GGUF models:</p> <pre><code>export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\nilab model evaluate --benchmark mt_bench --model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --judge-model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf\n...\n# SKILL EVALUATION REPORT\n\n## MODEL\n/home/example/.local/share/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n\n### AVERAGE:\n5.0 (across 1)\n\n### TURN ONE:\n5.0\n\n### TURN TWO:\nN/A\n\n### ERROR RATE:\n0.99\n</code></pre> <p>Note</p> <p>Currently, MTBench must be used with local models. Using models directly from Hugging Face without downloading them is unsupported.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#running-mtbenchbranch","title":"Running MTBenchBranch","text":"<p>Below is an example of running MTBenchBranch with a local safetensors model directory:</p> <pre><code>export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\nexport ILAB_TAXONOMY_DIR=$HOME/.local/share/instructlab/taxonomy\nilab model evaluate --benchmark mt_bench_branch \\\n --model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \\\n --judge-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \\\n --base-model $ILAB_MODELS_DIR/instructlab/granite-7b-lab \\\n --taxonomy-path $ILAB_TAXONOMY_DIR \\\n --branch rc \\\n --base-branch main\n...\n# SKILL EVALUATION REPORT\n\n## BASE MODEL\n/home/example/.local/share/instructlab/models/instructlab/granite-7b-lab\n\n## MODEL\n/home/example/.local/share/instructlab/models/instructlab/granite-7b-lab\n\n### IMPROVEMENTS:\n1. compositional_skills/extraction/receipt/markdown/qna.yaml (+4.0)\n2. compositional_skills/STEM/science/units_conversion/temperature_conversion/qna.yaml (+3.0)\n3. compositional_skills/extraction/commercial_lease_agreement/bullet_points/qna.yaml (+3.0)\n...\n\n### REGRESSIONS:\n1. compositional_skills/extraction/abstractive/title/qna.yaml (-5.0)\n2. compositional_skills/extraction/receipt/bullet_points/qna.yaml (-4.5)\n3. compositional_skills/writing/grounded/summarization/wiki_insights/one_line/qna.yaml (-4.0)\n...\n\n### NO CHANGE:\n1. compositional_skills/STEM/math/reasoning/qna.yaml\n2. compositional_skills/extraction/commercial_lease_agreement/csv/qna.yaml\n3. compositional_skills/roleplay/explain_like_i_am/graduate/qna.yaml\n...\n\n### NEW:\n1. compositional_skills/linguistics/organize_lists/qna.yaml\n2. compositional_skills/extraction/invoice/plain_text/qna.yaml\n3. compositional_skills/writing/grounded/summarization/wiki_insights/concise/qna.yaml\n...\n\n### ERROR RATE:\n0.32\n</code></pre> <p>Below is an example of running MTBenchBranch with local GGUF models:</p> <pre><code>export ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\nexport ILAB_TAXONOMY_DIR=$HOME/.local/share/instructlab/taxonomy\nilab model evaluate --benchmark mt_bench_branch --model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --judge-model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --base-model $ILAB_MODELS_DIR/granite-7b-lab-Q4_K_M.gguf --taxonomy-path $ILAB_TAXONOMY_DIR --branch rc --base-branch main\n...\n# SKILL EVALUATION REPORT\n\n## BASE MODEL\n/home/ec2-user/.local/share/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n\n## MODEL\n/home/ec2-user/.local/share/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n\n### NO CHANGE:\n1. compositional_skills/STEM/math/distance_conversion/qna.yaml\n\n### NEW:\n1. compositional_skills/linguistics/organize_lists/qna.yaml\n2. compositional_skills/extraction/annual_report/reasoning/qna.yaml\n3. compositional_skills/extraction/email/plain_text/qna.yaml\n4. compositional_skills/extraction/technical_paper/tables/bullet_points/qna.yaml\n5. compositional_skills/extraction/technical_paper/abstract/reasoning/qna.yaml\n\n### ERROR RATE:\n0.98\n</code></pre> <p>Note</p> <p>Currently, MTBenchBranch must be used with local models. Using models directly from Hugging Face without downloading them is unsupported.</p>"},{"location":"getting-started/creating_new_knowledge_or_skills/#serve-the-newly-trained-model","title":"\ud83c\udf74 Serve the newly trained model","text":"<p>1) Stop the server you have running by entering <code>ctrl+c</code> keys in the terminal running the server.</p> <p>Important</p> <p>\ud83c\udf4e This step is only implemented for macOS with M-series chips (for now).</p> <ul> <li>Before serving the newly trained model you must convert it to work with    the <code>ilab</code> cli. The <code>ilab model convert</code> command converts the new model into quantized GGUF format which is required by the server to host the model in the <code>ilab model serve</code> command.</li> </ul> <p>2) Convert the newly trained model by running the following command:</p> <pre><code>ilab model convert\n</code></pre> <p>3) Serve the newly trained model locally via <code>ilab model serve</code> command with the <code>--model-path</code> argument to specify your new model:</p> <pre><code>ilab model serve --model-path &lt;new model path&gt;\n</code></pre> <p>Which model should you select to serve? After running the <code>ilab model convert</code> command, some files and a directory are generated. The model you will want to serve ends with an extension of <code>.gguf</code>    and exists in a directory with the suffix <code>trained</code>. For example:    <code>instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf</code>.</p>"},{"location":"getting-started/linux_amd/","title":"Getting Started Linux AMD","text":""},{"location":"getting-started/linux_amd/#tldr","title":"tl;dr","text":"<p>Note</p> <p>These steps will pull down a premade <code>qna.yaml</code> so you can do a local build. Skip the <code>wget</code>, <code>mv</code>, and <code>ilab taxonomy diff</code> if you don't want to do this.</p> <pre><code>python3.11 -m venv venv-instructlab-0.18-3.11\nsource venv-instructlab-0.18-3.11/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[rocm]' \\\n   --extra-index-url https://download.pytorch.org/whl/rocm6.0 \\\n   -C cmake.args=\"-DLLAMA_HIPBLAS=on\" \\\n   -C cmake.args=\"-DAMDGPU_TARGETS=all\" \\\n   -C cmake.args=\"-DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang\" \\\n   -C cmake.args=\"-DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++\" \\\n   -C cmake.args=\"-DCMAKE_PREFIX_PATH=/opt/rocm\" \\\n   -C cmake.args=\"-DLLAMA_NATIVE=off\"\nwhich ilab\nilab config init\ncd ~/.local/share/instructlab\nmkdir -p taxonomy/knowledge/astronomy/constellations/Phoenix/\nwget https://raw.githubusercontent.com/instructlab/taxonomy/26b3fe21ccbb95adc06fe8ce76c7c18559e8dd05/knowledge/science/astronomy/constellations/phoenix/qna.yaml\nmv qna.yaml taxonomy/knowledge/astronomy/constellations/Phoenix/\nilab taxonomy diff\nilab data generate\nilab model train\nilab model convert --model-dir checkpoints/instructlab-granite-7b-lab-mlx-q\nilab model serve --model-path instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf\n</code></pre>"},{"location":"getting-started/linux_amd/#steps","title":"Steps","text":""},{"location":"getting-started/linux_amd/#install-ilab","title":"Install <code>ilab</code>","text":"<p>1) Create a new directory called <code>instructlab</code> to store the files the <code>ilab</code> CLI needs when running and <code>cd</code> into the directory by running the following command:</p> <pre><code>mkdir instructlab\ncd instructlab\n</code></pre> <p>Note</p> <p>The following steps in this document use Python venv for virtual environments. However, if you use another tool such as pyenv or Conda Miniforge for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in <code>venv</code>.</p> <p>2) There are a few ways you can locally install the <code>ilab</code> CLI. Select your preferred installation method from the following instructions. You can then install <code>ilab</code> and activate your <code>venv</code> environment.</p> <p>Note</p> <p>\u23f3 <code>pip install</code> may take some time, depending on your internet connection. In case installation fails with error <code>unsupported instruction `vpdpbusd'</code>, append <code>-C cmake.args=\"-DLLAMA_NATIVE=off\"</code> to <code>pip install</code> command.</p> <p>3) Install with AMD ROCm</p> <pre><code>python3 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[rocm]' \\\n   --extra-index-url https://download.pytorch.org/whl/rocm6.0 \\\n   -C cmake.args=\"-DLLAMA_HIPBLAS=on\" \\\n   -C cmake.args=\"-DAMDGPU_TARGETS=all\" \\\n   -C cmake.args=\"-DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang\" \\\n   -C cmake.args=\"-DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++\" \\\n   -C cmake.args=\"-DCMAKE_PREFIX_PATH=/opt/rocm\" \\\n   -C cmake.args=\"-DLLAMA_NATIVE=off\"\n</code></pre> <p>On Fedora 40+, use <code>-DCMAKE_C_COMPILER=clang-17</code> and <code>-DCMAKE_CXX_COMPILER=clang++-17.</code></p> <p>4) From your <code>venv</code> environment, verify <code>ilab</code> is installed correctly, by running the <code>ilab</code> command.</p> <pre><code>ilab\n</code></pre> <p>Example output of the <code>ilab</code> command</p> <pre><code>(venv) $ ilab\nUsage: ilab [OPTIONS] COMMAND [ARGS]...\n\nCLI for interacting with InstructLab.\n\nIf this is your first time running InstructLab, it's best to start with `ilab config init` to create the environment.\n\nOptions:\n--config PATH  Path to a configuration file.  [default:\n               /home/user/.config/instructlab/config.yaml]\n-v, --verbose  Enable debug logging (repeat for even more verbosity)\n--version      Show the version and exit.\n--help         Show this message and exit.\n\nCommands:\nconfig    Command Group for Interacting with the Config of InstructLab.\ndata      Command Group for Interacting with the Data generated by...\nmodel     Command Group for Interacting with the Models in InstructLab.\nsystem    Command group for all system-related command calls\ntaxonomy  Command Group for Interacting with the Taxonomy of InstructLab.\n\nAliases:\nchat      model chat\nconvert   model convert\ndiff      taxonomy diff\ndownload  model download\nevaluate  model evaluate\ngenerate  data generate\ninit      config init\nlist      model model_list\nserve     model serve\nsysinfo   system info\ntest      model test\ntrain     model train\n</code></pre> <p>Important</p> <p>Every <code>ilab</code> command needs to be run from within your Python virtual environment. You can enter the Python environment by running the <code>source venv/bin/activate</code> command.</p> <p>5) Optional: You can enable tab completion for the <code>ilab</code> command.</p>"},{"location":"getting-started/linux_amd/#bash-version-44-or-newer","title":"Bash (version 4.4 or newer)","text":"<p>Enable tab completion in <code>bash</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=bash_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=bash_source ilab &gt; ~/.ilab-complete.bash\necho \". ~/.ilab-complete.bash\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"getting-started/linux_amd/#zsh","title":"Zsh","text":"<p>Enable tab completion in <code>zsh</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=zsh_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.zshrc</code>:</p> <pre><code>_ILAB_COMPLETE=zsh_source ilab &gt; ~/.ilab-complete.zsh\necho \". ~/.ilab-complete.zsh\" &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"getting-started/linux_amd/#fish","title":"Fish","text":"<p>Enable tab completion in <code>fish</code> with the following command:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab | source\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab &gt; ~/.config/fish/completions/ilab.fish\n</code></pre>"},{"location":"getting-started/linux_amd/#initialize-ilab","title":"\ud83c\udfd7\ufe0f Initialize <code>ilab</code>","text":"<p>1) Initialize <code>ilab</code> by running the following command:</p> <pre><code>ilab config init\n</code></pre> <p>Example output</p> <pre><code>Welcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n</code></pre> <p>2) When prompted by the interface, press Enter to add a new default <code>config.yaml</code> file.</p> <p>3) When prompted, clone the <code>https://github.com/instructlab/taxonomy.git</code> repository into the current directory by typing y.</p> <p>Optional: If you want to point to an existing local clone of the <code>taxonomy</code> repository, you can pass the path interactively or alternatively with the <code>--taxonomy-path</code> flag.</p> <p>Example output after initializing <code>ilab</code></p> <pre><code>(venv) $ ilab config init\nWelcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y\nCloning https://github.com/instructlab/taxonomy.git...\n</code></pre> <p><code>ilab</code> will use the default configuration file unless otherwise specified. You can override this behavior with the <code>--config</code> parameter for any <code>ilab</code> command.</p> <p>4) When prompted, provide the path to your default model. Otherwise, the default of a quantized Merlinite model will be used - you can download this model with <code>ilab model download</code> (see below).</p> <pre><code>(venv) $ ilab config init\nWelcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y\nCloning https://github.com/instructlab/taxonomy.git...\nPath to your model [/home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]: &lt;ENTER&gt;\n</code></pre> <p>5) When prompted, please choose a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. YOU ARE ON LINUX, please choose <code>No Profile (CPU-Only)</code> by hitting Enter. There are various flags you can utilize with individual <code>ilab</code> commands that will allow you to utilize your GPU if applicable.</p> <pre><code>Welcome to InstructLab CLI. This guide will help you to setup your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [/home/user/.local/share/instructlab/taxonomy]:\nPath to your model [/home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:\nGenerating `/home/user/.config/instructlab/config.yaml`...\nPlease choose a train profile to use:\n[0] No profile (CPU-only)\n[1] A100_H100_x2.yaml\n[2] A100_H100_x4.yaml\n[3] A100_H100_x8.yaml\n[4] L40_x4.yaml\n[5] L40_x8.yaml\n[6] L4_x8.yaml\nEnter the number of your choice [hit enter for the default CPU-only profile] [0]:\nUsing default CPU-only train profile.\nInitialization completed successfully, you're ready to start using `ilab`. Enjoy!\n</code></pre> <p>The GPU profiles are listed by GPU type and number. If you happen to have a GPU configuration with a similar amount of VRAM as any of the above profiles, feel free to try them out!</p>"},{"location":"getting-started/linux_amd/#ilab-directory-layout-after-initializing-your-system","title":"<code>ilab</code> directory layout after initializing your system","text":"<p>After running <code>ilab config init</code> your directories will look like the following on a Linux system:</p> <pre><code>\u251c\u2500 ~/.cache/instructlab/models/ (1)\n\u251c\u2500 ~/.local/share/instructlab/datasets (2)\n\u251c\u2500 ~/.local/share/instructlab/taxonomy (3)\n\u251c\u2500 ~/.local/share/instructlab/checkpoints (4)\n</code></pre> <p>1) <code>~/.cache/instructlab/models/</code>: Contains all downloaded large language models, including the saved output of ones you generate with ilab. 2) <code>~/.local/share/instructlab/datasets/</code>: Contains data output from the SDG phase, built on modifications to the taxonomy repository. 3) <code>~/.local/share/instructlab/taxonomy/</code>: Contains the skill and knowledge data. 4) <code>~/.local/share/instructlab/checkpoints/</code>: Contains the output of the training process</p>"},{"location":"getting-started/linux_amd/#download-the-model","title":"\ud83d\udce5 Download the model","text":"<ul> <li>Run the <code>ilab model download</code> command.</li> </ul> <pre><code>ilab model download\n</code></pre> <p><code>ilab model download</code> downloads a compact pre-trained version of the model (~4.4G) from HuggingFace:</p> <pre><code>(venv) $ ilab model download\nDownloading model from Hugging Face: instructlab/merlinite-7b-lab-GGUF@main to /Users/USERNAME/Library/Caches/instructlab/models...\n...\nINFO 2024-08-01 15:05:48,464 huggingface_hub.file_download:1893: Download complete. Moving file to /home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf\n</code></pre> <p>Note</p> <p>\u23f3 This command can take few minutes or immediately depending on your internet connection or model is cached. If you have issues connecting to Hugging Face, refer to the Hugging Face discussion forum for more details.</p>"},{"location":"getting-started/linux_amd/#downloading-an-entire-hugging-face-repository-safetensors-model","title":"Downloading an entire Hugging Face repository (Safetensors Model)","text":"<ul> <li>Specify repository, and a Hugging Face token if necessary. For example:</li> </ul> <pre><code>HF_TOKEN=&lt;YOUR HUGGINGFACE TOKEN GOES HERE&gt; ilab model download --repository=TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF --filename=mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>These types of models are useful for GPU-enabled systems or anyone looking to serve a model using vLLM. InstructLab provides Safetensor versions of our Granite models on HuggingFace.</p>"},{"location":"getting-started/linux_amd/#listing-downloaded-models","title":"Listing downloaded models","text":"<p>All downloaded models can be seen with <code>ilab model list</code>.</p> <pre><code>ilab model list\n</code></pre> <p>Example output of <code>ilab model list</code> after <code>ilab model download</code></p> <pre><code>(venv) $ ilab model list\n+------------------------------+---------------------+--------+\n| Model Name                   | Last Modified       | Size   |\n+------------------------------+---------------------+--------+\n| merlinite-7b-lab-Q4_K_M.gguf | 2024-08-01 15:05:48 | 4.1 GB |\n+------------------------------+---------------------+--------+\n</code></pre>"},{"location":"getting-started/linux_amd/#serving-the-model","title":"\ud83c\udf74 Serving the model","text":"<ul> <li>Serve the model by running the following command:</li> </ul> <pre><code>ilab model serve\n</code></pre> <p>erve a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):</p> <pre><code>ilab model serve --model-path models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>nce the model is served and ready, you'll see the following output:</p> <pre><code>(venv) $ ilab model serve\nINFO 2024-03-02 02:21:11,352 lab.py:201 Using model 'models/ggml-merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.\nStarting server process\nAfter application startup complete see http://127.0.0.1:8000/docs for API.\nPress CTRL+C to shut down the server.\n</code></pre> <p>Note</p> <p>If multiple <code>ilab</code> clients try to connect to the same InstructLab server at the same time, the 1<sup>st</sup> will connect to the server while the others will start their own temporary server. This will require additional resources on the host machine.</p> <ul> <li>Serve a non-default Safetensors model (e.g. granite-7b-lab). NOTE: this requires a GPU.</li> </ul> <p>Ensure vllm is installed:</p> <pre><code>pip show vllm\n</code></pre> <p>If it is not, please run:</p> <pre><code>pip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\n</code></pre> <pre><code>ilab model serve --model-path ~/.cache/instructlab/models/instructlab/granite-7b-lab\n</code></pre>"},{"location":"getting-started/linux_amd/#chat-with-the-model-optional","title":"\ud83d\udce3 Chat with the model (Optional)","text":"<p>Because you're serving the model in one terminal window, you will have to create a new window and re-activate your Python virtual environment to run <code>ilab model chat</code> command:</p> <pre><code>source venv/bin/activate\nilab model chat\n</code></pre> <p>Chat with a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):</p> <pre><code>source venv/bin/activate\nilab model chat --model models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>Please note that usage of <code>--model</code> necessitates that the existing server has that model. If not, you must exit the server. <code>--model</code> in <code>ilab model chat</code> has the ability to start a server on your behalf with the specified model if one is not already running on the port.</p> <p>Before you start adding new skills and knowledge to your model, you can check its baseline performance by asking it a question such as <code>what is the capital of Canada?</code>.</p> <p>Note</p> <p>The model needs to be trained with the generated synthetic data to use the new skills or knowledge</p> <pre><code>(venv) $ ilab model chat\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 system \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Welcome to InstructLab Chat w/ GGML-MERLINITE-7B-lab-Q4_K_M (type /h for help)                                                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n&gt;&gt;&gt; what is the capital of Canada                                                                                                                                                                                                 [S][default]\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ggml-merlinite-7b-lab-Q4_K_M \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The capital city of Canada is Ottawa. It is located in the province of Ontario, on the southern banks of the Ottawa River in the eastern portion of southern Ontario. The city serves as the political center for Canada, as it is home to \u2502\n\u2502 Parliament Hill, which houses the House of Commons, Senate, Supreme Court, and Cabinet of Canada. Ottawa has a rich history and cultural significance, making it an essential part of Canada's identity.                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 elapsed 12.008 seconds \u2500\u256f\n</code></pre>"},{"location":"getting-started/linux_nvidia/","title":"Getting Started Linux NVidia","text":""},{"location":"getting-started/linux_nvidia/#tldr","title":"tl;dr","text":"<p>Note</p> <p>These steps will pull down a premade <code>qna.yaml</code> so you can do a local build. Skip the <code>wget</code>, <code>mv</code>, and <code>ilab taxonomy diff</code> if you don't want to do this.</p> <pre><code>python3.11 -m venv venv-instructlab-0.18-3.11\nsource venv-instructlab-0.18-3.11/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[cuda]' \\\n   -C cmake.args=\"-DLLAMA_CUDA=on\" \\\n   -C cmake.args=\"-DLLAMA_NATIVE=off\"\npip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\nwhich ilab\nilab config init\ncd ~/.local/share/instructlab\nmkdir -p taxonomy/knowledge/astronomy/constellations/Phoenix/\nwget https://raw.githubusercontent.com/instructlab/taxonomy/26b3fe21ccbb95adc06fe8ce76c7c18559e8dd05/knowledge/science/astronomy/constellations/phoenix/qna.yaml\nmv qna.yaml taxonomy/knowledge/astronomy/constellations/Phoenix/\nilab taxonomy diff\nilab data generate\nilab model train\nilab model convert --model-dir checkpoints/instructlab-granite-7b-lab-mlx-q\nilab model serve --model-path instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf\n</code></pre>"},{"location":"getting-started/linux_nvidia/#steps","title":"Steps","text":""},{"location":"getting-started/linux_nvidia/#install-ilab","title":"Install <code>ilab</code>","text":"<p>1) Create a new directory called <code>instructlab</code> to store the files the <code>ilab</code> CLI needs when running and <code>cd</code> into the directory by running the following command:</p> <pre><code>mkdir instructlab\ncd instructlab\n</code></pre> <p>Note</p> <p>The following steps in this document use Python venv for virtual environments. However, if you use another tool such as pyenv or Conda Miniforge for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in <code>venv</code>.</p> <p>2) There are a few ways you can locally install the <code>ilab</code> CLI. Select your preferred installation method from the following instructions. You can then install <code>ilab</code> and activate your <code>venv</code> environment.</p> <p>Note</p> <p>\u23f3 <code>pip install</code> may take some time, depending on your internet connection. In case installation fails with error <code>unsupported instruction `vpdpbusd'</code>, append <code>-C cmake.args=\"-DLLAMA_NATIVE=off\"</code> to <code>pip install</code> command.</p> <p>3) Install with Nvidia CUDA</p> <p>For the best CUDA experience, installing vLLM is necessary to serve Safetensors format models.</p> <pre><code>python3 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[cuda]' \\\n   -C cmake.args=\"-DLLAMA_CUDA=on\" \\\n   -C cmake.args=\"-DLLAMA_NATIVE=off\"\npip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\n</code></pre> <p>4) From your <code>venv</code> environment, verify <code>ilab</code> is installed correctly, by running the <code>ilab</code> command.</p> <pre><code>ilab\n</code></pre> <p>Example output of the <code>ilab</code> command</p> <pre><code>(venv) $ ilab\nUsage: ilab [OPTIONS] COMMAND [ARGS]...\n\nCLI for interacting with InstructLab.\n\nIf this is your first time running InstructLab, it's best to start with `ilab config init` to create the environment.\n\nOptions:\n--config PATH  Path to a configuration file.  [default:\n               /home/user/.config/instructlab/config.yaml]\n-v, --verbose  Enable debug logging (repeat for even more verbosity)\n--version      Show the version and exit.\n--help         Show this message and exit.\n\nCommands:\nconfig    Command Group for Interacting with the Config of InstructLab.\ndata      Command Group for Interacting with the Data generated by...\nmodel     Command Group for Interacting with the Models in InstructLab.\nsystem    Command group for all system-related command calls\ntaxonomy  Command Group for Interacting with the Taxonomy of InstructLab.\n\nAliases:\nchat      model chat\nconvert   model convert\ndiff      taxonomy diff\ndownload  model download\nevaluate  model evaluate\ngenerate  data generate\ninit      config init\nlist      model model_list\nserve     model serve\nsysinfo   system info\ntest      model test\ntrain     model train\n</code></pre> <p>Important</p> <p>Every <code>ilab</code> command needs to be run from within your Python virtual environment. You can enter the Python environment by running the <code>source venv/bin/activate</code> command.</p> <p>5) Optional: You can enable tab completion for the <code>ilab</code> command.</p>"},{"location":"getting-started/linux_nvidia/#bash-version-44-or-newer","title":"Bash (version 4.4 or newer)","text":"<p>Enable tab completion in <code>bash</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=bash_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=bash_source ilab &gt; ~/.ilab-complete.bash\necho \". ~/.ilab-complete.bash\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"getting-started/linux_nvidia/#zsh","title":"Zsh","text":"<p>Enable tab completion in <code>zsh</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=zsh_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.zshrc</code>:</p> <pre><code>_ILAB_COMPLETE=zsh_source ilab &gt; ~/.ilab-complete.zsh\necho \". ~/.ilab-complete.zsh\" &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"getting-started/linux_nvidia/#fish","title":"Fish","text":"<p>Enable tab completion in <code>fish</code> with the following command:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab | source\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab &gt; ~/.config/fish/completions/ilab.fish\n</code></pre>"},{"location":"getting-started/linux_nvidia/#initialize-ilab","title":"\ud83c\udfd7\ufe0f Initialize <code>ilab</code>","text":"<p>1) Initialize <code>ilab</code> by running the following command:</p> <pre><code>ilab config init\n</code></pre> <p>Example output</p> <pre><code>Welcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n</code></pre> <p>2) When prompted by the interface, press Enter to add a new default <code>config.yaml</code> file.</p> <p>3) When prompted, clone the <code>https://github.com/instructlab/taxonomy.git</code> repository into the current directory by typing y.</p> <p>Optional: If you want to point to an existing local clone of the <code>taxonomy</code> repository, you can pass the path interactively or alternatively with the <code>--taxonomy-path</code> flag.</p> <p>Example output after initializing <code>ilab</code></p> <pre><code>(venv) $ ilab config init\nWelcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y\nCloning https://github.com/instructlab/taxonomy.git...\n</code></pre> <p><code>ilab</code> will use the default configuration file unless otherwise specified. You can override this behavior with the <code>--config</code> parameter for any <code>ilab</code> command.</p> <p>4) When prompted, provide the path to your default model. Otherwise, the default of a quantized Merlinite model will be used - you can download this model with <code>ilab model download</code> (see below).</p> <pre><code>(venv) $ ilab config init\nWelcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y\nCloning https://github.com/instructlab/taxonomy.git...\nPath to your model [/home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]: &lt;ENTER&gt;\n</code></pre> <p>5) When prompted, please choose a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. YOU ARE ON LINUX, please choose <code>No Profile (CPU-Only)</code> by hitting Enter. There are various flags you can utilize with individual <code>ilab</code> commands that will allow you to utilize your GPU if applicable.</p> <pre><code>Welcome to InstructLab CLI. This guide will help you to setup your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [/home/user/.local/share/instructlab/taxonomy]:\nPath to your model [/home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:\nGenerating `/home/user/.config/instructlab/config.yaml`...\nPlease choose a train profile to use:\n[0] No profile (CPU-only)\n[1] A100_H100_x2.yaml\n[2] A100_H100_x4.yaml\n[3] A100_H100_x8.yaml\n[4] L40_x4.yaml\n[5] L40_x8.yaml\n[6] L4_x8.yaml\nEnter the number of your choice [hit enter for the default CPU-only profile] [0]:\nUsing default CPU-only train profile.\nInitialization completed successfully, you're ready to start using `ilab`. Enjoy!\n</code></pre> <p>The GPU profiles are listed by GPU type and number. If you happen to have a GPU configuration with a similar amount of VRAM as any of the above profiles, feel free to try them out!</p>"},{"location":"getting-started/linux_nvidia/#ilab-directory-layout-after-initializing-your-system","title":"<code>ilab</code> directory layout after initializing your system","text":"<p>After running <code>ilab config init</code> your directories will look like the following on a Linux system:</p> <pre><code>\u251c\u2500 ~/.cache/instructlab/models/ (1)\n\u251c\u2500 ~/.local/share/instructlab/datasets (2)\n\u251c\u2500 ~/.local/share/instructlab/taxonomy (3)\n\u251c\u2500 ~/.local/share/instructlab/checkpoints (4)\n</code></pre> <p>1) <code>~/.cache/instructlab/models/</code>: Contains all downloaded large language models, including the saved output of ones you generate with ilab. 2) <code>~/.local/share/instructlab/datasets/</code>: Contains data output from the SDG phase, built on modifications to the taxonomy repository. 3) <code>~/.local/share/instructlab/taxonomy/</code>: Contains the skill and knowledge data. 4) <code>~/.local/share/instructlab/checkpoints/</code>: Contains the output of the training process</p>"},{"location":"getting-started/linux_nvidia/#download-the-model","title":"\ud83d\udce5 Download the model","text":"<ul> <li>Run the <code>ilab model download</code> command.</li> </ul> <pre><code>ilab model download\n</code></pre> <p><code>ilab model download</code> downloads a compact pre-trained version of the model (~4.4G) from HuggingFace:</p> <pre><code>(venv) $ ilab model download\nDownloading model from Hugging Face: instructlab/merlinite-7b-lab-GGUF@main to /Users/USERNAME/Library/Caches/instructlab/models...\n...\nINFO 2024-08-01 15:05:48,464 huggingface_hub.file_download:1893: Download complete. Moving file to /home/user/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf\n</code></pre> <p>Note</p> <p>\u23f3 This command can take few minutes or immediately depending on your internet connection or model is cached. If you have issues connecting to Hugging Face, refer to the Hugging Face discussion forum for more details.</p>"},{"location":"getting-started/linux_nvidia/#downloading-an-entire-hugging-face-repository-safetensors-model","title":"Downloading an entire Hugging Face repository (Safetensors Model)","text":"<ul> <li>Specify repository, and a Hugging Face token if necessary. For example:</li> </ul> <pre><code>HF_TOKEN=&lt;YOUR HUGGINGFACE TOKEN GOES HERE&gt; ilab model download --repository=TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF --filename=mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>These types of models are useful for GPU-enabled systems or anyone looking to serve a model using vLLM. InstructLab provides Safetensor versions of our Granite models on HuggingFace.</p>"},{"location":"getting-started/linux_nvidia/#listing-downloaded-models","title":"Listing downloaded models","text":"<p>All downloaded models can be seen with <code>ilab model list</code>.</p> <pre><code>ilab model list\n</code></pre> <p>Example output of <code>ilab model list</code> after <code>ilab model download</code></p> <pre><code>(venv) $ ilab model list\n+------------------------------+---------------------+--------+\n| Model Name                   | Last Modified       | Size   |\n+------------------------------+---------------------+--------+\n| merlinite-7b-lab-Q4_K_M.gguf | 2024-08-01 15:05:48 | 4.1 GB |\n+------------------------------+---------------------+--------+\n</code></pre>"},{"location":"getting-started/linux_nvidia/#serving-the-model","title":"\ud83c\udf74 Serving the model","text":"<ul> <li>Serve the model by running the following command:</li> </ul> <pre><code>ilab model serve\n</code></pre> <p>erve a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):</p> <pre><code>ilab model serve --model-path models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>nce the model is served and ready, you'll see the following output:</p> <pre><code>(venv) $ ilab model serve\nINFO 2024-03-02 02:21:11,352 lab.py:201 Using model 'models/ggml-merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.\nStarting server process\nAfter application startup complete see http://127.0.0.1:8000/docs for API.\nPress CTRL+C to shut down the server.\n</code></pre> <p>Note</p> <p>If multiple <code>ilab</code> clients try to connect to the same InstructLab server at the same time, the 1<sup>st</sup> will connect to the server while the others will start their own temporary server. This will require additional resources on the host machine.</p> <ul> <li>Serve a non-default Safetensors model (e.g. granite-7b-lab). NOTE: this requires a GPU.</li> </ul> <p>Ensure vllm is installed:</p> <pre><code>pip show vllm\n</code></pre> <p>If it is not, please run:</p> <pre><code>pip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\n</code></pre> <pre><code>ilab model serve --model-path ~/.cache/instructlab/models/instructlab/granite-7b-lab\n</code></pre>"},{"location":"getting-started/linux_nvidia/#chat-with-the-model-optional","title":"\ud83d\udce3 Chat with the model (Optional)","text":"<p>Because you're serving the model in one terminal window, you will have to create a new window and re-activate your Python virtual environment to run <code>ilab model chat</code> command:</p> <pre><code>source venv/bin/activate\nilab model chat\n</code></pre> <p>Chat with a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):</p> <pre><code>source venv/bin/activate\nilab model chat --model models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>Please note that usage of <code>--model</code> necessitates that the existing server has that model. If not, you must exit the server. <code>--model</code> in <code>ilab model chat</code> has the ability to start a server on your behalf with the specified model if one is not already running on the port.</p> <p>Before you start adding new skills and knowledge to your model, you can check its baseline performance by asking it a question such as <code>what is the capital of Canada?</code>.</p> <p>Note</p> <p>The model needs to be trained with the generated synthetic data to use the new skills or knowledge</p> <pre><code>(venv) $ ilab model chat\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 system \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Welcome to InstructLab Chat w/ GGML-MERLINITE-7B-lab-Q4_K_M (type /h for help)                                                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n&gt;&gt;&gt; what is the capital of Canada                                                                                                                                                                                                 [S][default]\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ggml-merlinite-7b-lab-Q4_K_M \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The capital city of Canada is Ottawa. It is located in the province of Ontario, on the southern banks of the Ottawa River in the eastern portion of southern Ontario. The city serves as the political center for Canada, as it is home to \u2502\n\u2502 Parliament Hill, which houses the House of Commons, Senate, Supreme Court, and Cabinet of Canada. Ottawa has a rich history and cultural significance, making it an essential part of Canada's identity.                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 elapsed 12.008 seconds \u2500\u256f\n</code></pre>"},{"location":"getting-started/mac_metal/","title":"Getting Started Mac Metal","text":""},{"location":"getting-started/mac_metal/#tldr","title":"tl;dr","text":"<p>Note</p> <p>These steps will pull down a premade <code>qna.yaml</code> so you can do a local build. Skip the <code>wget</code>, <code>mv</code>, and <code>ilab taxonomy diff</code> if you don't want to do this.</p> <pre><code>python3.11 -m venv venv-instructlab-0.18-3.11\nsource venv-instructlab-0.18-3.11/bin/activate\npip install 'instructlab[mps]'\nwhich ilab\nilab config init\ncd ~/Library/Application\\ Support/instructlab/\nmkdir -p taxonomy/knowledge/astronomy/constellations/Phoenix/\nwget https://raw.githubusercontent.com/instructlab/taxonomy/26b3fe21ccbb95adc06fe8ce76c7c18559e8dd05/knowledge/science/astronomy/constellations/phoenix/qna.yaml\nmv qna.yaml taxonomy/knowledge/astronomy/constellations/Phoenix/\nilab taxonomy diff\nilab data generate\nilab model train\nilab model convert --model-dir checkpoints/instructlab-granite-7b-lab-mlx-q\nilab model serve --model-path instructlab-granite-7b-lab-trained/instructlab-granite-7b-lab-Q4_K_M.gguf\n</code></pre>"},{"location":"getting-started/mac_metal/#steps","title":"Steps","text":""},{"location":"getting-started/mac_metal/#install-ilab","title":"Install <code>ilab</code>","text":"<p>1) Create a new directory called <code>instructlab</code> to store the files the <code>ilab</code> CLI needs when running and <code>cd</code> into the directory by running the following command:</p> <pre><code>mkdir instructlab\ncd instructlab\n</code></pre> <p>Note</p> <p>The following steps in this document use Python venv for virtual environments. However, if you use another tool such as pyenv or Conda Miniforge for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in <code>venv</code>.</p> <p>2) There are a few ways you can locally install the <code>ilab</code> CLI. Select your preferred installation method from the following instructions. You can then install <code>ilab</code> and activate your <code>venv</code> environment.</p> <p>Note</p> <p>\u23f3 <code>pip install</code> may take some time, depending on your internet connection. In case installation fails with error <code>unsupported instruction `vpdpbusd'</code>, append <code>-C cmake.args=\"-DLLAMA_NATIVE=off\"</code> to <code>pip install</code> command.</p> <p>3) Install with Apple Metal on M1/M2/M3 Macs</p> <p>Note</p> <p>Make sure your system Python build is <code>Mach-O 64-bit executable arm64</code> by using <code>file -b $(command -v python)</code>, or if your system is setup with pyenv by using the <code>file -b $(pyenv which python)</code> command.</p> <pre><code>python3 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\npip install 'instructlab[mps]'\n</code></pre> <p>4) From your <code>venv</code> environment, verify <code>ilab</code> is installed correctly, by running the <code>ilab</code> command.</p> <pre><code>ilab\n</code></pre> <p>Example output of the <code>ilab</code> command</p> <pre><code>(venv) $ ilab\nUsage: ilab [OPTIONS] COMMAND [ARGS]...\n\nCLI for interacting with InstructLab.\n\nIf this is your first time running InstructLab, it's best to start with `ilab config init` to create the environment.\n\nOptions:\n--config PATH  Path to a configuration file.  [default:\n               /home/user/.config/instructlab/config.yaml]\n-v, --verbose  Enable debug logging (repeat for even more verbosity)\n--version      Show the version and exit.\n--help         Show this message and exit.\n\nCommands:\nconfig    Command Group for Interacting with the Config of InstructLab.\ndata      Command Group for Interacting with the Data generated by...\nmodel     Command Group for Interacting with the Models in InstructLab.\nsystem    Command group for all system-related command calls\ntaxonomy  Command Group for Interacting with the Taxonomy of InstructLab.\n\nAliases:\nchat      model chat\nconvert   model convert\ndiff      taxonomy diff\ndownload  model download\nevaluate  model evaluate\ngenerate  data generate\ninit      config init\nlist      model model_list\nserve     model serve\nsysinfo   system info\ntest      model test\ntrain     model train\n</code></pre> <p>Important</p> <p>Every <code>ilab</code> command needs to be run from within your Python virtual environment. You can enter the Python environment by running the <code>source venv/bin/activate</code> command.</p> <p>5) Optional: You can enable tab completion for the <code>ilab</code> command.</p>"},{"location":"getting-started/mac_metal/#bash-version-44-or-newer","title":"Bash (version 4.4 or newer)","text":"<p>Enable tab completion in <code>bash</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=bash_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=bash_source ilab &gt; ~/.ilab-complete.bash\necho \". ~/.ilab-complete.bash\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"getting-started/mac_metal/#zsh","title":"Zsh","text":"<p>Enable tab completion in <code>zsh</code> with the following command:</p> <pre><code>eval \"$(_ILAB_COMPLETE=zsh_source ilab)\"\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.zshrc</code>:</p> <pre><code>_ILAB_COMPLETE=zsh_source ilab &gt; ~/.ilab-complete.zsh\necho \". ~/.ilab-complete.zsh\" &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"getting-started/mac_metal/#fish","title":"Fish","text":"<p>Enable tab completion in <code>fish</code> with the following command:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab | source\n</code></pre> <p>To have this enabled automatically every time you open a new shell, you can save the completion script and source it from <code>~/.bashrc</code>:</p> <pre><code>_ILAB_COMPLETE=fish_source ilab &gt; ~/.config/fish/completions/ilab.fish\n</code></pre>"},{"location":"getting-started/mac_metal/#initialize-ilab","title":"\ud83c\udfd7\ufe0f Initialize <code>ilab</code>","text":"<p>1) Initialize <code>ilab</code> by running the following command:</p> <pre><code>ilab config init\n</code></pre> <p>Example output</p> <pre><code>Welcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n</code></pre> <p>2) When prompted by the interface, press Enter to add a new default <code>config.yaml</code> file.</p> <p>3) When prompted, clone the <code>https://github.com/instructlab/taxonomy.git</code> repository into the current directory by typing y.</p> <p>Optional: If you want to point to an existing local clone of the <code>taxonomy</code> repository, you can pass the path interactively or alternatively with the <code>--taxonomy-path</code> flag.</p> <p>Example output after initializing <code>ilab</code></p> <pre><code>(venv) $ ilab config init\nWelcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y\nCloning https://github.com/instructlab/taxonomy.git...\n</code></pre> <p><code>ilab</code> will use the default configuration file unless otherwise specified. You can override this behavior with the <code>--config</code> parameter for any <code>ilab</code> command.</p> <p>4) When prompted, provide the path to your default model. Otherwise, the default of a quantized Merlinite model will be used - you can download this model with <code>ilab model download</code> (see below).</p> <pre><code>(venv) $ ilab config init\nWelcome to InstructLab CLI. This guide will help you set up your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [taxonomy]: &lt;ENTER&gt;\n`taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y\nCloning https://github.com/instructlab/taxonomy.git...\nPath to your model [/Users/USERNAME/Library/Caches/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]: &lt;ENTER&gt;\n</code></pre> <p>5) When prompted, please choose a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. YOU ARE ON MacOS, please choose <code>No Profile (CPU-Only)</code> by hitting Enter. There are various flags you can utilize with individual <code>ilab</code> commands that will allow you to utilize your GPU if applicable.</p> <pre><code>Welcome to InstructLab CLI. This guide will help you to setup your environment.\nPlease provide the following values to initiate the environment [press Enter for defaults]:\nPath to taxonomy repo [~/Library/Application\\ Support/instructlab/taxonomy]:\nPath to your model [/Users/USERNAME/Library/Caches//instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:\nGenerating `~/Library/Application\\ Support/instructlab/config.yaml`...\nPlease choose a train profile to use:\n[0] No profile (CPU-only)\n[1] A100_H100_x2.yaml\n[2] A100_H100_x4.yaml\n[3] A100_H100_x8.yaml\n[4] L40_x4.yaml\n[5] L40_x8.yaml\n[6] L4_x8.yaml\nEnter the number of your choice [hit enter for the default CPU-only profile] [0]:\nUsing default CPU-only train profile.\nInitialization completed successfully, you're ready to start using `ilab`. Enjoy!\n</code></pre> <p>The GPU profiles are listed by GPU type and number. If you happen to have a GPU configuration with a similar amount of VRAM as any of the above profiles, feel free to try them out!</p>"},{"location":"getting-started/mac_metal/#ilab-directory-layout-after-initializing-your-system","title":"<code>ilab</code> directory layout after initializing your system","text":"<p>After running <code>ilab config init</code> your directories will look like the following on a Linux system:</p> <pre><code>\u251c\u2500 ~/Library/Application\\ Support/instructlab/models/ (1)\n\u251c\u2500 ~/Library/Application\\ Support/instructlab/datasets (2)\n\u251c\u2500 ~/Library/Application\\ Support/instructlab/taxonomy (3)\n\u251c\u2500 ~/Library/Application\\ Support/instructlab/checkpoints (4)\n</code></pre> <p>1) <code>/Users/USERNAME/Library/Caches/instructlab/models/</code>: Contains all downloaded large language models, including the saved output of ones you generate with ilab.  2) <code>~/Library/Application\\ Support/instructlab/datasets/</code>: Contains data output from the SDG phase, built on modifications to the taxonomy repository.  3) <code>~/Library/Application\\ Support/instructlab/taxonomy/</code>: Contains the skill and knowledge data.  4) <code>~/Users/USERNAME/Library/Caches/instructlab/checkpoints/</code>: Contains the output of the training process</p>"},{"location":"getting-started/mac_metal/#download-the-model","title":"\ud83d\udce5 Download the model","text":"<ul> <li>Run the <code>ilab model download</code> command.</li> </ul> <pre><code>ilab model download\n</code></pre> <p><code>ilab model download</code> downloads a compact pre-trained version of the model (~4.4G) from HuggingFace:</p> <pre><code>(venv) $ ilab model download\nDownloading model from Hugging Face: instructlab/merlinite-7b-lab-GGUF@main to /Users/USERNAME/Library/Caches/instructlab/models...\n...\nINFO 2024-08-01 15:05:48,464 huggingface_hub.file_download:1893: Download complete. Moving file to /Users/USERNAME/Library/Caches/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf\n</code></pre> <p>Note</p> <p>\u23f3 This command can take few minutes or immediately depending on your internet connection or model is cached. If you have issues connecting to Hugging Face, refer to the Hugging Face discussion forum for more details.</p>"},{"location":"getting-started/mac_metal/#downloading-an-entire-hugging-face-repository-safetensors-model","title":"Downloading an entire Hugging Face repository (Safetensors Model)","text":"<ul> <li>Specify repository, and a Hugging Face token if necessary. For example:</li> </ul> <pre><code>HF_TOKEN=&lt;YOUR HUGGINGFACE TOKEN GOES HERE&gt; ilab model download --repository=instructlab/granite-7b-lab\n</code></pre> <p>These types of models are useful for GPU-enabled systems or anyone looking to serve a model using vLLM. InstructLab provides Safetensor versions of our Granite models on HuggingFace.</p>"},{"location":"getting-started/mac_metal/#listing-downloaded-models","title":"Listing downloaded models","text":"<p>All downloaded models can be seen with <code>ilab model list</code>.</p> <pre><code>ilab model list\n</code></pre> <p>Example output of <code>ilab model list</code> after <code>ilab model download</code></p> <pre><code>(venv) $ ilab model list\n+------------------------------+---------------------+--------+\n| Model Name                   | Last Modified       | Size   |\n+------------------------------+---------------------+--------+\n| merlinite-7b-lab-Q4_K_M.gguf | 2024-08-01 15:05:48 | 4.1 GB |\n+------------------------------+---------------------+--------+\n</code></pre>"},{"location":"getting-started/mac_metal/#serving-the-model","title":"\ud83c\udf74 Serving the model","text":"<ul> <li>Serve the model by running the following command:</li> </ul> <pre><code>ilab model serve\n</code></pre> <p>erve a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):</p> <pre><code>ilab model serve --model-path models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>nce the model is served and ready, you'll see the following output:</p> <pre><code>(venv) $ ilab model serve\nINFO 2024-03-02 02:21:11,352 lab.py:201 Using model 'models/ggml-merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.\nStarting server process\nAfter application startup complete see http://127.0.0.1:8000/docs for API.\nPress CTRL+C to shut down the server.\n</code></pre> <p>Note</p> <p>If multiple <code>ilab</code> clients try to connect to the same InstructLab server at the same time, the 1<sup>st</sup> will connect to the server while the others will start their own temporary server. This will require additional resources on the host machine.</p> <ul> <li>Serve a non-default Safetensors model (e.g. granite-7b-lab). NOTE: this requires a GPU.</li> </ul> <p>Ensure vllm is installed:</p> <pre><code>pip show vllm\n</code></pre> <p>If it is not, please run:</p> <pre><code>pip install vllm@git+https://github.com/opendatahub-io/vllm@2024.08.01\n</code></pre> <pre><code>ilab model serve --model-path ~/.cache/instructlab/models/instructlab/granite-7b-lab\n</code></pre>"},{"location":"getting-started/mac_metal/#chat-with-the-model-optional","title":"\ud83d\udce3 Chat with the model (Optional)","text":"<p>Because you're serving the model in one terminal window, you will have to create a new window and re-activate your Python virtual environment to run <code>ilab model chat</code> command:</p> <pre><code>source venv/bin/activate\nilab model chat\n</code></pre> <p>Chat with a non-default model (e.g. Mixtral-8x7B-Instruct-v0.1):</p> <pre><code>source venv/bin/activate\nilab model chat --model models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n</code></pre> <p>Please note that usage of <code>--model</code> necessitates that the existing server has that model. If not, you must exit the server. <code>--model</code> in <code>ilab model chat</code> has the ability to start a server on your behalf with the specified model if one is not already running on the port.</p> <p>Before you start adding new skills and knowledge to your model, you can check its baseline performance by asking it a question such as <code>what is the capital of Canada?</code>.</p> <p>Note</p> <p>The model needs to be trained with the generated synthetic data to use the new skills or knowledge</p> <pre><code>(venv) $ ilab model chat\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 system \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Welcome to InstructLab Chat w/ GGML-MERLINITE-7B-lab-Q4_K_M (type /h for help)                                                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n&gt;&gt;&gt; what is the capital of Canada                                                                                                                                                                                                 [S][default]\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ggml-merlinite-7b-lab-Q4_K_M \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The capital city of Canada is Ottawa. It is located in the province of Ontario, on the southern banks of the Ottawa River in the eastern portion of southern Ontario. The city serves as the political center for Canada, as it is home to \u2502\n\u2502 Parliament Hill, which houses the House of Commons, Senate, Supreme Court, and Cabinet of Canada. Ottawa has a rich history and cultural significance, making it an essential part of Canada's identity.                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 elapsed 12.008 seconds \u2500\u256f\n</code></pre>"},{"location":"resources/CONTRIBUTORS/","title":"Contributors","text":"<p>Lets celebrate our friends who help out with this site here!</p>"},{"location":"resources/MKDOCS/","title":"mkdocs examples","text":"<p>This page includes a few neat tricks that you can do with <code>mkdocs</code>. For a complete list of examples visit the mkdocs documentation.</p>"},{"location":"resources/MKDOCS/#code","title":"Code","text":"<pre><code>print(\"hello world!\")\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-line-numbers","title":"Code with line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-highlights","title":"Code with highlights","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-tabs","title":"Code with tabs","text":"Tab Header <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> Another Tab Header <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"resources/MKDOCS/#more-tabs","title":"More tabs","text":"Windows <p>If on windows download the <code>Win32.zip</code> file and install it.</p> MacOS <p>Run <code>brew install foo</code>.</p> Linux <p>Run <code>apt-get install foo</code>.</p>"},{"location":"resources/MKDOCS/#checklists","title":"Checklists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> In hac habitasse platea dictumst</li> </ul>"},{"location":"resources/MKDOCS/#add-a-button","title":"Add a button","text":"<p>Launch the lab</p> <p>Visit IBM Developer</p> <p>Sign up! </p>"},{"location":"resources/MKDOCS/#call-outs","title":"Call outs","text":"<p>Tip</p> <p>You can use <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code> <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>quote</code> or <code>example</code>.</p> <p>Note</p> <p>A note.</p> <p>Abstract</p> <p>An abstract.</p> <p>Info</p> <p>Some info.</p> <p>Success</p> <p>A success.</p> <p>Question</p> <p>A question.</p> <p>Warning</p> <p>A warning.</p> <p>Danger</p> <p>A danger.</p> <p>Example</p> <p>A example.</p> <p>Bug</p> <p>A bug.</p>"},{"location":"resources/MKDOCS/#call-outs-with-code","title":"Call outs with code","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.</p>"},{"location":"resources/MKDOCS/#formatting","title":"Formatting","text":"<p>In addition to the usual italics, and bold there is now support for:</p> <ul> <li>highlighted</li> <li>underlined</li> <li>strike-through</li> </ul>"},{"location":"resources/MKDOCS/#tables","title":"Tables","text":"OS or Application Username Password Windows VM <code>Administrator</code> <code>foo</code> Linux VM <code>root</code> <code>bar</code>"},{"location":"resources/MKDOCS/#emojis","title":"Emojis","text":"<p>Yes, these work.  </p>"},{"location":"resources/MKDOCS/#images","title":"Images","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/MKDOCS/#right-align-image","title":"right align image","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/RESOURCES/","title":"Additional resources","text":"<p>TODO, but PRs accepted!</p>"},{"location":"taxonomy/","title":"Welcome to InstructLab's Taxonomy","text":""},{"location":"taxonomy/#welcome-to-the-instructlab-taxonomy","title":"Welcome to the InstructLab Taxonomy","text":"<p>InstructLab \ud83d\udc36 uses a novel synthetic data-based alignment tuning method for Large Language Models (LLMs.) The \"lab\" in Instruct**Lab** \ud83d\udc36 stands for **L**arge-Scale **A**lignment for Chat**B**ots [1].</p> <p>The LAB method is driven by taxonomies, which are largely created manually and with care.</p> <p>This repository contains a taxonomy tree that allows you to create models tuned with your data (enhanced via synthetic data generation) using the LAB \ud83d\udc36 method.</p> <p>[1] Shivchander Sudalairaj*, Abhishek Bhandwaldar*, Aldo Pareja*, Kai Xu, David D. Cox, Akash Srivastava*. \"LAB: Large-Scale Alignment for ChatBots\", arXiv preprint arXiv: 2403.01081, 2024. (* denotes equal contributions)</p>"},{"location":"taxonomy/#choosing-domains-for-the-taxonomy","title":"Choosing domains for the taxonomy","text":"<p>In general, we use the Dewey Decimal Classification (DDC) System to determine our domains (and subdomains) in the taxonomy. This DDC SUMMARIES document is a great resource for determining where a topic might be classified.</p> <p>If you are unsure where to put your knowledge or compositional skill, create a folder in the <code>miscellaneous_unknown</code> folder under the <code>knowledge</code> or <code>compositional_skills</code> folders.</p>"},{"location":"taxonomy/#learning","title":"Learning","text":"<p>Learn about the concepts of \"skills\" and \"knowledge\" in our InstructLab Community Learning Guide.</p>"},{"location":"taxonomy/#taxonomy-tree-layout","title":"Taxonomy tree Layout","text":"<p>The taxonomy tree is organized in a cascading directory structure. At the end of each branch, there is a YAML file (qna.yaml) that contains the examples for that domain. Maintainers can decide to change the names of the existing branches or to add new branches.</p> <p>Important</p> <p>Folder names do not have spaces. Use underscores between words.</p>"},{"location":"taxonomy/#taxonomy-diagram","title":"Taxonomy diagram","text":"<p>Note</p> <p>These diagrams shows a subset of the taxonomy. It is not a complete representation.</p> <pre><code> flowchart TD;\n   na[not accepting contributions\\n at this time]:::na\n   taxonomy --&gt; foundational_skill &amp; compositional_skills &amp; knowledge\n\n   foundational_skill:::na --&gt; reasoning:::na\n   reasoning:::na --&gt; common_sense_reasoning:::na\n   reasoning:::na --&gt; mathematical_reasoning:::na\n   reasoning:::na --&gt; theory_of_mind:::na\n\n   compositional_skills --&gt; engineering\n   compositional_skills --&gt; grounded\n   compositional_skills --&gt; lingustics\n\n   grounded --&gt; grounded/arts\n   grounded --&gt; grounded/geography\n   grounded --&gt; grounded/history\n   grounded --&gt; grounded/science\n\n   knowledge --&gt; knowledge/arts\n\n   knowledge --&gt; knowledge/miscellaneous_unknown\n   knowledge --&gt; knowledge/science\n   knowledge --&gt; knowledge/technology\n   knowledge/science --&gt; animals --&gt; birds --&gt; black_capped_chickadee --&gt; black_capped_chikadee-a &amp; black_capped_chikadee-q\n   knowledge/science --&gt; astronomy --&gt; constellations --&gt; phoenix --&gt; phoenix-a &amp; phoenix-q\n\n   black_capped_chikadee-a{attribution.txt}\n   black_capped_chikadee-q{qna.yaml}\n   phoenix-a{attribution.txt}\n   phoenix-q{qna.yaml}\n   classDef na fill:#EEE</code></pre> <p>Below is an illustrative directory structure to show this layout:</p> <pre><code>.\n\u2514\u2500\u2500 linguistics\n    \u251c\u2500\u2500 writing\n    \u2502   \u251c\u2500\u2500 brainstorming\n    \u2502   \u2502   \u251c\u2500\u2500 idea_generation\n    |   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2502   \u2502   \u251c\u2500\u2500 refute_claim\n    |   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2502   \u251c\u2500\u2500 prose\n    \u2502   \u2502   \u251c\u2500\u2500 articles\n    \u2502   \u2502       \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502           attribution.txt\n    \u2514\u2500\u2500 grammar\n        \u2514\u2500\u2500 qna.yaml\n        \u2502   attribution.txt\n        \u2514\u2500\u2500 spelling\n            \u2514\u2500\u2500 qna.yaml\n                attribution.txt\n</code></pre>"},{"location":"taxonomy/#contribute-knowledge-and-skills-to-the-taxonomy","title":"Contribute knowledge and skills to the taxonomy","text":"<p>The ability to contribute to a Large Language Model (LLM) has been difficult in no small part because it is difficult to get access to the necessary compute infrastructure.</p> <p>This taxonomy repository will be used as the seed to synthesize the training data for InstructLab-trained models. We intend to retrain the model(s) using the main branch following InstructLab's progressive training on a regular basis. This enables fast iteration of the model(s), for the benefit of the open source community.</p> <p>By contributing your skills and knowledge to this repository, you will see your changes built into an LLM within days of your contribution rather than months or years! If you are working with a model and notice its knowledge or ability lacking, you can correct it by contributing knowledge or skills and check if it's improved after your changes are built.</p> <p>While public contributions are welcome to help drive community progress, you can also fork this repository under the Apache License, Version 2.0, add your own internal skills, and train your own models internally. However, you might need your own access to significant compute infrastructure to perform sufficient retraining.</p>"},{"location":"taxonomy/#ways-to-contribute","title":"Ways to Contribute","text":"<p>You can contribute to the taxonomy in the following two ways:</p> <ol> <li>Adding new examples to existing leaf nodes:</li> <li>Adding new branches/skills corresponding to the existing domain:</li> </ol> <p>For more information, see the Ways of contributing to the taxonomy repository documentation.</p>"},{"location":"taxonomy/#how-to-contribute-skills-and-knowledge","title":"How to contribute skills and knowledge","text":"<p>To contribute to this repo, you'll use the Fork and Pull model common in many open source repositories. You can add your skills and knowledge to the taxonomy in multiple ways; for additional information on how to make a contribution, see the Documentation on contributing. You can also use the following guides to help with contributing:</p> <ul> <li>Contributing using the GitHub webpage UI.</li> <li>Contributing knowledge to the taxonomy in the Knowledge contribution guidelines.</li> </ul>"},{"location":"taxonomy/#why-should-i-contribute","title":"Why should I contribute?","text":"<p>This taxonomy repository will be used as the seed to synthesize the training data for InstructLab-trained models. We intend to retrain the model(s) using the main branch as often as possible (at least weekly). Fast iteration of the model(s) benefits the open source community and enables model developers who do not have access to the necessary compute infrastructure.</p>"},{"location":"taxonomy/knowledge/","title":"Getting Started with Knowledge Contributions","text":""},{"location":"taxonomy/knowledge/#getting-started-with-knowledge-contributions","title":"Getting Started with Knowledge Contributions","text":"<p>While skills are foundational or performative, knowledge is based more on answering questions that involve facts, data, or references.</p> <p>Knowledge is supported by documents, such as a textbook, technical manual, encyclopedia, journal, or magazine.</p> <p>Knowledge in the taxonomy tree consists of a few more elements than skills:</p> <ul> <li>Each knowledge node in the tree has a <code>qna.yaml</code>, similar to the format of the <code>qna.yaml</code> for skills.</li> <li>\u2b50 Knowledge submissions require you to create a Git repository, can be with GitHub, that contains the markdown files of your knowledge contributions. These contributions in your repository must use the markdown (.md) format.</li> <li>The <code>qna.yaml</code> includes parameters that contain information from your repository.</li> </ul> <p>Tip</p> <p>Guidelines for Knowledge contributions</p> <ul> <li>Submit the most up-to-date version of the document</li> <li>All submissions must be text, images will be ignored</li> <li>Do not use tables in your markdown freeform contribution</li> </ul> <p>The <code>qna.yaml</code> format must include the following fields:</p> <ul> <li><code>version</code>: The version of the qna.yaml file, this is the format of the file used for SDG. The value must be the number 3.</li> <li><code>created_by</code>: Your GitHub username.</li> <li><code>domain</code>: Specify the category of the knowledge.</li> <li><code>seed_examples</code>: A collection of key/value entries.</li> <li><code>context</code>: A chunk of information from the knowledge document. Each <code>qna.yaml</code> needs five <code>context</code> blocks and has a maximum word count of 500 words.</li> <li><code>questions_and_answers</code>: The parameter that holds your questions and answers<ul> <li><code>question</code>: Specify a question for the model. Each <code>qna.yaml</code> file needs at least three question and answer pairs per <code>context</code> chunk with a maximum word count of 250 words.</li> <li><code>answer</code>: Specify the desired answer from the model. Each <code>qna.yaml</code> file needs at least three question and answer pairs per <code>context</code> chunk with a maximum word count of 250 words.</li> </ul> </li> <li><code>document_outline</code>: Describe an overview of the document your submitting.</li> <li><code>document</code>: The source of your knowledge contribution.</li> <li><code>repo</code>: The URL to your repository that holds your knowledge markdown files.</li> <li><code>commit</code>: The SHA of the commit in your repository with your knowledge markdown files.</li> <li><code>patterns</code>: A list of glob patterns specifying the markdown files in your repository. Any glob pattern that starts with <code>*</code>, such as <code>*.md</code>, must be quoted due to YAML rules. For example, <code>\"*.md\"</code>.</li> </ul>"},{"location":"taxonomy/knowledge/#knowledge-yaml-examples","title":"Knowledge: YAML examples","text":"<pre><code>version: 3\ndomain: astronomy\ncreated_by: juliadenham\nseed_examples:\n  - context: |\n      **Phoenix** is a minor constellation in the southern sky. Named after the mythical\n      phoenix, it was first depicted on a celestial atlas by Johann Bayer in his 1603\n      *Uranometria*. The French explorer and astronomer Nicolas Louis de\n      Lacaille charted the brighter stars and gave their Bayer designations\n      in 1756. The constellation stretches from roughly \u221239 degrees to \u221257 degrees\n      declination, and from 23.5h to 2.5h of right ascension. The constellations Phoenix,\n      Grus, Pavo, are known as the Southern Birds.\n    questions_and_answers:\n      - question: |\n          What is the Phoenix constellation?\n        answer: |\n          Phoenix is a minor constellation in the southern sky.\n      - question: |\n          Who charted the Phoenix constellation?\n        answer: |\n          The Phoenix constellation was charted by french explorer and\n          astronomer Nicolas Louis de Lacaille.\n      - question: |\n          How far does the Phoenix constellation stretch?\n        answer: |\n          The phoenix constellation stretches from roughly \u221239\u00b0 to \u221257\u00b0\n          declination, and from 23.5h to 2.5h of right ascension.\n  - context: |\n      Phoenix was the largest of the 12 constellations established by Petrus\n      Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de\n      Houtman. It first appeared on a 35cm diameter celestial globe published\n      in 1597 (or 1598) in Amsterdam by Plancius with Jodocus Hondius. The first\n      depiction of this constellation in a celestial atlas was in Johann Bayer's\n      *Uranometria* of 1603. De Houtman included it in his southern star catalog\n      the same year under the Dutch name *Den voghel Fenicx*, \"The Bird Phoenix\",\n      symbolising the phoenix of classical mythology. One name of the brightest star Alpha\n      Phoenicis\u2014Ankaa\u2014is derived from the Arabic: \u0627\u0644\u0639\u0646\u0642\u0627\u0621, romanized: al-\u2018anq\u0101\u2019,\n      lit.\u2009'the phoenix', and was coined sometime after 1800 in relation to the constellation.\n    questions_and_answers:\n      - question: |\n          What is the brightest star in the Phoenix constellation\n          called?\n        answer: |\n          Alpha Phoenicis or Ankaa is the brightest star in the Phoenix\n          Constellation.\n      - question: Where did the Phoenix constellation first appear?\n        answer: |\n          The Phoenix constellation first appeared on a 35-cm diameter\n          celestial globe published in 1597 (or 1598) in Amsterdam by\n          Plancius with Jodocus Hondius.\n      - question: |\n          What does \"The Bird Phoenix\" symbolize?\n        answer: |\n          \"The Bird Phoenix\" symbolizes the phoenix of classical mythology.\n  - context: |\n      Phoenix is a small constellation bordered by Fornax and Sculptor to the north,\n      Grus to the west, Tucana to the south, touching on the corner of Hydrus to the\n      south, and Eridanus to the east and southeast. The bright star Achernar is\n      nearby. The three-letter abbreviation for the constellation, as adopted by the\n      International Astronomical Union in 1922, is \"Phe\". The official constellation\n      boundaries, as set by Belgian astronomer Eug\u00e8ne Delporte in 1930,\n      are defined by a polygon of 10 segments. In the equatorial coordinate system, the right\n      ascension coordinates of these borders lie between 23h 26.5m and 02h 25.0m,\n      while the declination coordinates are between \u221239.31\u00b0 and \u221257.84\u00b0. This means it remains\n      below the horizon to anyone living north of the 40th parallel in the Northern\n      Hemisphere, and remains low in the sky for anyone living north of the equator.\n      It is most visible from locations such as Australia and South Africa during\n      late Southern Hemisphere spring. Most of the constellation lies within, and\n      can be located by, forming a triangle of the bright stars Achernar, Fomalhaut\n      and Beta Ceti\u2014Ankaa lies roughly in the centre of this.\n    questions_and_answers:\n      - question: What are the characteristics of the Phoenix constellation?\n        answer: |\n          Phoenix is a small constellation bordered by Fornax and Sculptor to\n          the north, Grus to the west, Tucana to the south, touching on the\n          corner of Hydrus to the south, and Eridanus to the east and southeast.\n          The bright star Achernar is nearby.\n      - question: |\n          When is the phoenix constellation most visible?\n        answer: |\n          Phoenix is most visible from locations such as Australia and\n          South Africa during late Southern Hemisphere spring.\n      - question: |\n          What are the Phoenix Constellation boundaries?\n        answer: |\n          The official constellation boundaries for Phoenix, as set by Belgian\n          astronomer Eug\u00e8ne Delporte in 1930, are defined by a polygon of 10\n          segments.\n  - context: |\n      Ten stars have been found to have planets to date, and four planetary\n      systems have been discovered with the SuperWASP project. HD 142 is a yellow\n      giant that has an apparent magnitude of 5.7, and has a planet HD 142b 1.36\n      times the mass of Jupiter which orbits every 328 days.  HD 2039 is a yellow\n      subgiant with an apparent magnitude of 9.0 around 330 light years away which\n      has a planet HD 2039 b six times the mass of Jupiter. WASP-18 is a star of\n      magnitude 9.29 which was discovered to have a hot Jupiter-like planet taking\n      less than a day to orbit the star. The planet is suspected to be causing WASP-18 to\n      appear older than it really is. WASP-4 and WASP-5 are solar-type yellow stars around 1000\n      light years distant and of 13th magnitude, each with a single planet larger\n      than Jupiter. WASP-29 is an orange dwarf of spectral type K4V and visual magnitude\n      11.3, which has a planetary companion of similar size and mass to Saturn. The planet\n      completes an orbit every 3.9 days.\n    questions_and_answers:\n      - question: In the Phoenix constellation, how many stars have planets?\n        answer: |\n          In the Phoenix constellation, ten stars have been found to have\n          planets to date, and four planetary systems have been discovered\n          with the SuperWASP project.\n      - question: |\n          What is HD 142?\n        answer: |\n          HD 142 is a yellow giant that has an apparent magnitude of 5.7, and\n          has a planet (HD 142 b) 1.36 times the mass of Jupiter which\n          orbits every 328 days.\n      - question: |\n          Are WASP-4 and WASP-5 solar-type yellow stars?\n        answer: |\n          Yes, WASP-4 and WASP-5 are solar-type yellow stars around 1000 light\n          years distant and of 13th magnitude, each with a single planet\n          larger than Jupiter.\n  - context: |\n      The constellation does not lie on the galactic plane of the Milky Way, and there\n      are no prominent star clusters. NGC 625 is a dwarf irregular galaxy of apparent magnitude 11.0\n      and lying some 12.7 million light years distant. Only 24000 light years in\n      diameter, it is an outlying member of the Sculptor Group. NGC 625 is\n      thought to have been involved in a collision and is experiencing a burst\n      of active star formation. NGC 37 is a lenticular galaxy of apparent magnitude\n      14.66. It is approximately 42 kiloparsecs in diameter and about 12.9 billion years old.\n      Robert's Quartet , and three spiral galaxies NGC 88 and NGC 92) is a group of\n      four galaxies located around 160 million light-years away which are in the process of colliding\n      and merging. They are within a circle of radius of 1.6 arcmin, corresponding to about\n      75,000 light-years. Located in the galaxy ESO 243-49 is HLX-1, an intermediate-mass\n      black hole intermediate-mass_black_hole \u2014the first one of its kind identified.\n      It is thought to be a remnant of a dwarf galaxy that was absorbed in a collision\n      with ESO 243-49. Before its discovery, this class of black hole was only hypothesized.\n    questions_and_answers:\n      - question: |\n          Is the Phoenix Constellation part of the Milky Way?\n        answer: |\n          The Phoenix constellation does not lie on the galactic plane of\n          the Milky Way, and there are no prominent star clusters.\n      - question: |\n          How many light years away is NGC 625?\n        answer: |\n          NGC 625 is 24000 light years in diameter and is an outlying\n          member of the Sculptor Group.\n      - question: |\n          What is Robert's Quartet composed of?\n        answer: |\n          Robert's Quartet is composed of the irregular galaxy NGC 87,\n          and three spiral galaxies NGC 88, NGC 89 and NGC 92.\ndocument_outline: |\n  Information about the Phoenix Constellation including the\n  history, characteristics, and features of the stars in the constellation.\ndocument:\n    repo: https://github.com/juliadenham/Summit_knowledge\n    commit: 0a1f2672b9b90582e6115333e3ed62fd628f1c0f\n    patterns:\n      - phoenix_constellation.md\n</code></pre> <p>Example <code>attribution.txt</code> file</p> <pre><code>Title of work: Phoenix (constellation)\nLink to work: https://en.wikipedia.org/wiki/Phoenix_(constellation)\nRevision: https://en.wikipedia.org/w/index.php?title=Phoenix_(constellation)&amp;oldid=1237187773\nLicense of the work: CC-BY-SA-4.0\nCreator names: Wikipedia Authors\n</code></pre> <p>This knowledge example references one markdown file: <code>phoenix_constellation.md</code>. You can also add multiple files for knowledge contributions.</p> <p>Note</p> <p>Due to the higher volume, it will naturally take longer to receive acceptance for a knowledge contribution pull request than for a skill pull request. Smaller pull requests are simpler and require less time and effort to review.</p> <p>What might these markdown files look like? They can be freeform. Here's what a snippet of <code>phoenix_constellation.md</code> might look like in your Git repository.</p>"},{"location":"taxonomy/knowledge/#knowledge-markdown-file-example","title":"Knowledge: Markdown file example","text":"<pre><code># Phoenix (constellation)\n\n**Phoenix** is a minor constellation in the southern sky. Named after the mythical\nphoenix, it was first depicted on a celestial atlas by Johann Bayer in his 1603\n*Uranometria*. The French explorer and astronomer Nicolas Louis de\nLacaille charted the brighter stars and gave their Bayer designations\nin 1756. The constellation stretches from roughly \u221239 degrees to \u221257 degrees\ndeclination, and from 23.5h to 2.5h of right ascension. The constellations Phoenix,\nGrus, Pavo, are known as the Southern Birds.\n\nThe brightest star, Alpha Phoenicis, is named Ankaa, an Arabic word meaning 'the Phoenix'.\nIt is an orange giant of apparent magnitude 2.4. Next is Beta Phoenicis, actually a\nbinary system composed of two yellow giants with a combined apparent magnitude of 3.3. Nu\nPhoenicis has a dust disk, while the constellation has ten star systems with known planets and the recently\ndiscovered galaxy clusters El Gordo and the Phoenix\nCluster\u2014located 7.2 and 5.7 billion light years away respectively, two of the largest objects in the visible\nuniverse. Phoenix is the radiant of two annual meteor showers: the Phoenicids in December, and the July\nPhoenicids.\n</code></pre> <p>In the taxonomy repository, here's what the previously referenced knowledge might look like in the tree:</p>"},{"location":"taxonomy/knowledge/#knowledge-directory-tree-example","title":"Knowledge: directory tree example","text":"<pre><code>[...]\n\n\u2514\u2500\u2500 knowledge\n    \u2514\u2500\u2500 science\n        \u251c\u2500\u2500 astronomy\n        \u2502 \u2514\u2500\u2500 constellations\n        \u2502     \u2514\u2500\u2500 Phoenix &lt;=== here it is :)\n        \u2502     |    \u2514\u2500\u2500 qna.yaml\n        |     |        attribution.txt\n        \u2502     \u2514\u2500\u2500 Orion\n        \u2502          \u2514\u2500\u2500 qna.yaml\n        |              attribution.txt\n[...]\n</code></pre> <p>For more information on what to include in your <code>attribution.txt</code> file, see For your attribution.txt file in CONTRIBUTING.md.</p> <p>You can organize the knowledge markdown files in your repository however you want. You just need to ensure the YAML is pointing to the correct file.</p>"},{"location":"taxonomy/knowledge/contribution_details/","title":"Knowledge Contribution Guidelines","text":"<p>You can create a Git repository to host your knowledge contributions anywhere (GitLab, Gerrit, etc.) but it may be favorable to create one on GitHub. The following instructions show you how to create a knowledge repository in GitHub and contribute to the taxonomy.</p>"},{"location":"taxonomy/knowledge/contribution_details/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a GitHub account</li> <li>You have a forked copy of the taxonomy repository</li> <li>Verify that the model does not already know the knowledge you want to submit</li> </ul>"},{"location":"taxonomy/knowledge/contribution_details/#creating-your-own-knowledge-repository","title":"Creating your own knowledge repository","text":"<p>To create a new GitHub repository, follow the GitHub documentation in Creating a new repository.</p> <p>The specific steps are listed as follows:</p> <ol> <li>In your GitHub profile page, navigate to the repositories tab. You will see a search bar where you can search your repositories, or create a new one.</li> <li>This takes you to a page titled \u201cCreate a new repository\u201d. Create a custom name for your repository and add a README.md file. For example, \u201cknowlege_contributions\u201d could be a good name for your repository.</li> <li>Click \u201cCreate\u201d when you are all set.</li> </ol>"},{"location":"taxonomy/knowledge/contribution_details/#convert-your-knowledge-documentation-to-markdown","title":"Convert your knowledge documentation to markdown","text":"<p>There are many online tools that can help you convert your documents to markdown. If you are using a wiki page for your contributions, you can use pandocs to convert the documents. For wikipedia sources on pandoc, use <code>from: mediawiki</code> and convert <code>to: markdown_strict</code> to access the proper markdown format.</p>"},{"location":"taxonomy/knowledge/contribution_details/#add-the-markdown-file-to-your-repository","title":"Add the markdown file to your repository","text":"<p>To add a file to your GitHub repository, follow the GitHub documentation in Adding a file to a repository.</p> <p>The specific steps are listed as follows:</p> <ol> <li>Navigate to \u201cAdd files\u201d. Click \u201cCreate new file\u201d if you want to manually add your markdown content. Click \u201cUpload files\u201d if you have a file locally to add.</li> <li> <p>Add a description and commit your changes.</p> <p>Since this is your own repository, you can commit directly to the <code>main</code> branch.</p> </li> <li> <p>You can then see your new content in your repository.</p> </li> </ol> <p>Important</p> <p>Make a note of your commit SHA; you need it for your <code>qna.yaml</code>.</p>"},{"location":"taxonomy/knowledge/contribution_details/#create-a-pull-request-in-the-taxonomy-repository","title":"Create a pull request in the taxonomy repository","text":"<p>Navigate to your forked taxonomy repository and ensure it is up-to-date.</p> <p>There are a few ways you can create a pull request:</p> <ul> <li>For details on the local process, check out The GitHub Workflow Guide in the kubernetes documentation and the GitHub flow in the GitHub documentation.</li> <li>For details on contributing using the GitHub webpage UI, see Contributing using the GH UI or Creating a pull request in the GitHub documentation.</li> </ul>"},{"location":"taxonomy/knowledge/contribution_details/#verification","title":"Verification","text":"<p>Here are a few things to check before seeking reviews for your contribution:</p> <ul> <li>Your <code>qna.yaml</code> follows the proper formatting. See examples in Knowledge: YAML examples</li> <li>Ensure all parameters are set. Especially the <code>document</code>, <code>repo</code>, <code>commit</code> and <code>pattern</code> keys; these parameters are specific to knowledge contributions and require more analysis.</li> <li>Include an <code>attribution.txt</code> file for citing your sources. see For your attribution.txt file for more information.</li> </ul>"},{"location":"taxonomy/knowledge/contribution_details/#pr-upstream-workflow","title":"PR Upstream Workflow","text":"<p>The following table outlines the expected timing for the PR(s) you have put in. The PRs go through a few steps, and checks, but you should be able to map your <code>label</code> to the place that it is in.</p> Label Actor Action Duration Contributor Submit PR - Contributor Fix failed PR checks - https://github.com/instructlab/taxonomy/labels/triage-needed Triager Review PR, ask for changes Days https://github.com/instructlab/taxonomy/labels/triage-requested-changes Contributor Make requested changes Days https://github.com/instructlab/taxonomy/labels/precheck-generate-ready Triager Run prechecks and generate Days https://github.com/instructlab/taxonomy/labels/community-build-ready Backend Model gets retrained Weeks Triager Check the numbers and PR merged or closed -"},{"location":"taxonomy/knowledge/guide/","title":"What is \"Knowledge\"?","text":"<p>Knowledge consists of data and facts and is backed by documents. When you create knowledge for a model, you're giving it additional data to more accurately answer questions.</p> <p>Knowledge contributions in this project contain a few things.</p> <ul> <li>A file in a git repository that holds your information. For example, these repositories can include markdown versions of information on: Oscar 2024 winners, Law books, Shakespeare, Sports, Chemistry, etc.</li> <li>A <code>qna.yaml</code> file that asks and answers questions about the information in the git repository.</li> <li>A <code>attribution.txt</code> that includes the sources for the information used in the <code>qna.yaml</code>.</li> </ul> <p>You can learn more about the knowledge structure in Getting Started with Knowledge contributions.</p>"},{"location":"taxonomy/knowledge/guide/#accepted-knowledge","title":"Accepted Knowledge","text":"<p>Important</p> <p>We are currently only accepting knowledge contributions as a limited private beta and sources will be limited to articles from Wikipedia.</p> <p>There are a few domains of knowledge that we are currently accepting. For a full list of knowledge fields, see Knowledge domains in the taxonomy documentation</p> <p>A few examples are as follows:</p>"},{"location":"taxonomy/knowledge/guide/#stem-fields","title":"STEM fields","text":"<ul> <li>Physics</li> <li>Astronomy and Astrophysics</li> <li>Quantum Mechanics</li> <li> <p>Special Relativity and General Relativity</p> </li> <li> <p>Chemistry &amp; Chemical Engineering</p> </li> <li>Organic Chemistry</li> <li>Inorganic Chemistry</li> <li>Chemical engineering</li> <li> <p>Biotechnology</p> </li> <li> <p>Earth &amp; Environmental Science</p> </li> <li>Geology</li> <li> <p>Geography</p> </li> <li> <p>Biology &amp; Life Sciences</p> </li> <li>Plants (Botany)</li> <li> <p>Medicine &amp; health</p> </li> <li> <p>Electrical Engineering</p> </li> <li>Bioengineering</li> <li>Civil Engineering</li> <li>Industrial Engineering</li> </ul>"},{"location":"taxonomy/knowledge/guide/#legal-and-regulatory","title":"Legal and regulatory","text":"<ul> <li>Intellectual Property</li> <li>Criminal Law</li> <li>Civil Rights</li> <li>Healthcare compliance</li> </ul>"},{"location":"taxonomy/knowledge/guide/#economy-and-business","title":"Economy and Business","text":"<ul> <li>Economy and Businesses</li> <li>Accounting and Finance</li> <li>Marketing</li> <li>Human Resource</li> <li>Management</li> </ul>"},{"location":"taxonomy/knowledge/guide/#philosophy","title":"Philosophy","text":"<ul> <li>Philosophy</li> <li>Metaphysics</li> <li>Epistemology</li> <li>Ethics</li> <li>Parapsychology &amp; occultism</li> <li>Philosophical schools of thought</li> </ul>"},{"location":"taxonomy/knowledge/guide/#literature","title":"Literature","text":"<ul> <li>Literature, rhetoric &amp; criticism</li> <li>American literature in English</li> <li>Other literatures</li> </ul>"},{"location":"taxonomy/knowledge/guide/#avoid-these-topics","title":"Avoid These Topics","text":"<p>While the tuning process may eventually benefit from being used to help the models work with complex social topics, at this time this is an area of active research we do not want to take lightly. Therefore please keep your submissions clear of the following topics:</p> <ul> <li>PII (personally identifiable information) or any content invasive of individual privacy rights</li> <li>Violence including self-harm</li> <li>Cyber Bullying</li> <li>Internal documentation or other that is confidential to your employer or organization, e.g. trade secrets</li> <li>Discrimination</li> <li>Religion</li> <li>Facts such as, \"Christianity is, according to the 2011 census, the fifth most practiced religion in Nepal, with 375,699 adherents, or 1.4% of the population\", are fine as a knowledge contribution. Advocating in favor of or against any religious faith is not acceptable.</li> <li>Medical or health information</li> <li>Facts such as,  \"In mammals, pulmonary ventilation occurs via inhalation (breathing),\" are fine as a knowledge contribution. Tailored medical/health advice is not acceptable.</li> <li>Financial information</li> <li>Facts such as \"laissez-faire economics ... argues that market forces alone should drive the economy and that governments should refrain from direct intervention in or moderation of the economic system,\" are fine as a knowledge contribution. Tailored financial advice is not acceptable.</li> <li>Legal settlements/mitigations</li> <li>Gender Bias</li> <li>Hostile Language, threats, slurs, derogatory or insensitive jokes or comments</li> <li>Profanity</li> <li>Pornography and sexually explicit or suggestive content</li> <li>Any contributions that would allow for automated decision making that affect an individual's rights or well-being, e.g. social scoring</li> <li>Any contributions that engage in political campaigning or lobbying</li> </ul> <p>We are also not accepting submissions of the following content:</p> <ul> <li>Code</li> <li>Anything code-related that can be traced back to code for a computer. Not limited to <code>sed</code> or <code>bash</code> but <code>yaml</code>s for OpenShift or Kubernetes, to <code>python</code> snippets to <code>Java</code> suggestions. There are specific models focused on this space and this isn't for this model for the time being.</li> <li>Jokes</li> <li>Poems</li> </ul> <p>We received many joke and poem submissions at the beginning of the project, and with jokes being \"in the eye of the beholder\" and puns requiring nuance for native English speakers, we realized we were possibly unconsciously biasing our model. We have discovered that working with both topics has its own challenges, and if we want something generalized, finding consensus was unsuccessful. For now, we're not accepting additional submissions of jokes and poems.</p>"},{"location":"taxonomy/knowledge/guide/#building-your-llm-intuition","title":"Building Your LLM Intuition","text":"<p>LLMs have inherent limitations that make certain tasks extremely difficult, like doing math problems. They're great at other tasks, like creative writing. And they could be better at things like logical reasoning.</p> <p>An LLM with knowledge helps it create a basis of information that it can learn from, then you can teach it to use this knowledge via the <code>qna.yaml</code> files.</p> <p>For example, you can give an LLM the entire periodic table, then in a <code>qna.yaml</code> add something like:</p> <pre><code>question: What is the symbol and atomic number for Chlorine?\nanswer: |\n  The symbol for chlorine is Cl and the atomic number is 17.\n</code></pre> <p>With a few of these qna's, the model will learn the periodic table because it has the knowledge data.</p>"},{"location":"taxonomy/knowledge/guide/#llms-are-great-at","title":"LLMs are great at","text":"<p>For these, however, it's common for LLMs to already have excellent performance. Try 3-5 examples in <code>lab chat</code> to confirm a deficit in the model before you build your submission, and share the examples in your Pull Request (PR).</p> <ul> <li>Brainstorming</li> <li>Creativity</li> <li>Connecting information</li> <li>Cross-lingual behavior</li> </ul>"},{"location":"taxonomy/knowledge/guide/#llms-need-help-with","title":"LLMs need help with","text":"<p>LLM behavior in these sorts of topics are very difficult for the model to get right. Try several examples to understand the nuances of the model's ability to do these sorts of tasks, and consider using corrections to the results you get in your tuning process.</p> <ul> <li>Chains of reasoning</li> <li>Analysis</li> <li>Story plots</li> <li>Reassembling information</li> <li>Effective and succinct summaries</li> </ul>"},{"location":"taxonomy/knowledge/guide/#llms-are-not-so-great-at","title":"LLMs are not so great at","text":"<p>LLMs may struggle with solving math and computation. That said, improving some of these foundational skills may be something this work tackles in the future, but not at this time.</p> <ul> <li>Math</li> <li>Computation</li> <li>\"Turing-complete\" type tasks</li> <li>Generating only true real-world information (they're prone to hallucinations)</li> </ul>"},{"location":"taxonomy/skills/","title":"Getting Started with Skill Contributions","text":""},{"location":"taxonomy/skills/#getting-started-with-skill-contributions","title":"Getting Started with Skill Contributions","text":"<p>Skills require a much smaller volume of content than knowledge contributions. An entire skill contribution to the taxonomy tree can be just a few lines of YAML in the <code>qna.yaml</code> file (\"qna\" is short for \"questions and answers\") and an <code>attribution.txt</code> file for citing sources.</p> <p>Your skills contribution pull requests must include the following:</p> <ul> <li>A <code>qna.yaml</code> that contains a set of key/value entries with the following keys</li> <li>Each <code>qna.yaml</code> file requires a minimum of five question and answer pairs.</li> <li>An <code>attribution.txt</code> that includes the sources for the information used in the <code>qna.yaml</code></li> </ul> <p>Tip</p> <p>The skill taxonomy structure is used in several ways:</p> <ol> <li>To select the right subset of the taxonomy to use for data generation.</li> <li>To determine the interpretability by human contributors and maintainers.</li> <li>As part of the prompt to the LLM used to generate synthetic samples.</li> </ol> <p>Important</p> <p>There is a limit to how much content can exist in the question/answer pairs for the model to process. Due to this, only add a maximum of around 2300 words to your question and answer seed example pairs in the <code>qna.yaml</code> file.</p> <p>Compositional skills can either be grounded (includes a context) or ungrounded (does not include a context).  Grounded or ungrounded is declared in the taxonomy tree, for example: <code>linguistics/writing/poetry/haiku/</code> (ungrounded) or <code>grounded/linguistics/grammar</code> (grounded). The <code>qna.yaml</code> is in the final node.</p> <p>Taxonomy skill files must be a valid YAML file named <code>qna.yaml</code>. Each <code>qna.yaml</code> file contains a set of key/value entries with the following keys:</p> <ul> <li><code>version</code>: The value must be the number 2. Required</li> <li><code>task_description</code>: A description of the skill. Required</li> <li><code>created_by</code>: The GitHub username of the contributor. Required</li> <li><code>seed_examples</code>: A collection of key/value entries. New   submissions should have at least five entries, although   older files may have fewer. Required</li> <li><code>context</code>: Grounded skills require the user to provide context containing information that the model is expected to take into account during processing. This is different from knowledge, where the model is expected to gain facts and background knowledge from the tuning process. The context key should not be used for ungrounded skills.</li> <li><code>question</code>: A question for the model. Required</li> <li><code>answer</code>: The desired response from the model. Required</li> </ul> <p>Other keys at any level are currently ignored.</p>"},{"location":"taxonomy/skills/#skills-yaml-examples","title":"Skills: YAML examples","text":"<p>To make the <code>qna.yaml</code> files easier and faster for humans to read, it is recommended to specify <code>version</code> first, followed by <code>task_description</code>, then <code>created_by</code>, and finally <code>seed_examples</code>. In <code>seed_examples</code>, it is recommended to specify <code>context</code> first (if applicable), followed by <code>question</code> and <code>answer</code>.</p> <p>Example <code>qna.yaml</code></p> <pre><code>version: 2\ntask_description: &lt;string&gt;\ncreated_by: &lt;string&gt;\nseed_examples:\n  - question: &lt;string&gt;\n    answer: |\n      &lt;multi-line string&gt;\n  - context: |\n      &lt;multi-line string&gt;\n    question: &lt;string&gt;\n    answer: |\n      &lt;multi-line string&gt;\n  ...\n</code></pre> <p>Then, you create an <code>attribution.txt</code> file that includes the sources of your information. These can also be self authored sources.</p> <p>Example <code>attribution.txt</code></p> <pre><code>[Link to source]\n[Link to work]\n[License of the work]\n[Creator name]\n</code></pre> <p>For more information on what to include in your <code>attribution.txt</code> file, see For your attribution.txt file in CONTRIBUTING.md.</p> <p>If you have not written YAML before, don't be intimidated - it's just text.</p> <p>Tip</p> <ul> <li>Spaces and indentation matter in YAML. Two spaces to indent.</li> <li>Don't use tabs!</li> <li>Be careful to not have trailing spaces at the end of a line.</li> <li>Each example in <code>seed_examples</code> begins with a \"-\". Place this \"-\" in front of the first field (<code>question</code> or <code>context</code>). The remaining keys in the example should not have this \"-\".</li> <li>Some special characters such as \" and ' need to be escaped with backslash. This is why some of the lines for keys in the example YAML start the value with the '|' character followed a new line and then an indented multi-line string. This character disables all of the special characters in the value for the key. You might also want to use the '|' character for multi-line strings.</li> <li>Consider quoting all values with \" to avoid surprising YAML parser behavior (e.g. Yes answer can be interpreted by the parser as a boolean of <code>True</code> value, unless \"Yes\" is quoted.)</li> <li>See https://yaml-multiline.info/ for more info.</li> </ul> <p>It is recommended that you lint, or verify, your YAML using a tool. One linter option is yamllint.com. You can copy/paste your YAML into the box and click Go to have it analyze your YAML and make recommendations. Online tools like prettified and yaml-validator can automatically reformat your YAML to adhere to our <code>yamllint</code> PR checks, such as breaking lines longer than 120 characters.</p>"},{"location":"taxonomy/skills/#ungrounded-compositional-skill-yaml-example","title":"Ungrounded compositional skill: YAML example","text":"<pre><code>version: 2\ntask_description: 'Teach the model how to rhyme.'\ncreated_by: juliadenham\nseed_examples:\n  - question: What are 5 words that rhyme with horn?\n    answer: warn, torn, born, thorn, and corn.\n  - question: What are 5 words that rhyme with cat?\n    answer: bat, gnat, rat, vat, and mat.\n  - question: What are 5 words that rhyme with poor?\n    answer: door, shore, core, bore, and tore.\n  - question: What are 5 words that rhyme with bank?\n    answer: tank, rank, prank, sank, and drank.\n  - question: What are 5 words that rhyme with bake?\n    answer: wake, lake, steak, make, and quake.\n</code></pre> <p>Seriously, that's it.</p> <p>Here is the location of this YAML in the taxonomy tree. Note that the YAML file itself, plus any added directories that contain the file, is the entirety of the skill in terms of a taxonomy contribution:</p>"},{"location":"taxonomy/skills/#ungrounded-compositional-skill-directory-tree-example","title":"Ungrounded compositional skill: Directory tree example","text":"<pre><code>[...]\n\n\u2514\u2500\u2500 writing\n    \u2514\u2500\u2500 poetry\n    |   \u2514\u2500\u2500 haiku &lt;=== here it is :)\n    |   |   \u2514\u2500\u2500 qna.yaml\n    |   |       attribution.txt\n        [...]\n    \u2514\u2500\u2500 prose\n    |   \u2514\u2500\u2500 debate\n    |   |   \u2514\u2500\u2500 qna.yaml\n    |   |       attribution.txt\n    [...]\n\n[...]\n</code></pre>"},{"location":"taxonomy/skills/#grounded-compositional-skill-yaml-example","title":"Grounded compositional skill: YAML example","text":"<p>Remember that grounded compositional skills require additional context and include a <code>context</code> field.</p> <p>This example snippet assumes the GitHub username <code>mairin</code> and shows some of the question/answer pairs present in the actual file:</p> <pre><code>version: 2\ntask_description: |\n    This skill provides the ability to read a markdown-formatted table.\ncreated_by: mairin # Use your GitHub username; only one creator supported\nseed_examples:\n  - context: |\n      | **Breed**      | **Size**     | **Barking** | **Energy** |\n      |----------------|--------------|-------------|------------|\n      | Afghan Hound   | 25-27 in     | 3/5         | 4/5        |\n      | Labrador       | 22.5-24.5 in | 3/5         | 5/5        |\n      | Cocker Spaniel | 14.5-15.5 in | 3/5         | 4/5        |\n      | Poodle (Toy)   | &lt;= 10 in     | 4/5         | 4/5        |\n    question: |\n      Which breed has the most energy?\n    answer: |\n      The breed with the most energy is the Labrador.\n  - context: |\n      | **Name** | **Date** | **Color** | **Letter** | **Number** |\n      |----------|----------|-----------|------------|------------|\n      | George   | Mar 5    | Green     | A          | 1          |\n      | Gr\u00e1inne  | Dec 31   | Red       | B          | 2          |\n      | Abigail  | Jan 17   | Yellow    | C          | 3          |\n      | Bhavna   | Apr 29   | Purple    | D          | 4          |\n      | R\u00e9my     | Sep 9    | Blue      | E          | 5          |\n    question: |\n      What is Gr\u00e1inne's letter and what is her color?\n    answer: |\n      Gr\u00e1inne's letter is B and her color is red.\n  - context: |\n      | Banana | Apple      | Blueberry | Strawberry |\n      |--------|------------|-----------|------------|\n      | Yellow | Red, Green | Blue      | Red        |\n      | Large  | Medium     | Small     | Small      |\n      | Peel   | Peel       | No peel   | No peel    |\n    question: |\n      Which fruit is blue, small, and has no peel?\n    answer: |\n      The blueberry is blue, small, and has no peel.\n</code></pre>"},{"location":"taxonomy/skills/#grounded-compositional-skill-directory-tree-example","title":"Grounded compositional skill: Directory tree example","text":"<pre><code>[...]\n\ngrounded\n\u2514\u2500\u2500 technology\n    \u2514\u2500\u2500 machine_learning\n        \u2514\u2500\u2500 natural_language_processing\n    |   |     \u2514\u2500\u2500 information_extraction\n    |            \u2514\u2500\u2500 inference\n    |   |            \u2514\u2500\u2500 qualitative\n    |   |               \u251c\u2500\u2500 sentiment\n    |   |               |     \u2514\u2500\u2500 qna.yaml\n    |   |               |         attribution.txt\n    \u2502                   \u251c\u2500\u2500 quantitative\n    \u2502   \u2502                   \u251c\u2500\u2500 table_analysis &lt;=== here it is :)\n    \u2502   |   |               |     \u2514\u2500\u2500 qna.yaml\n    \u2502   \u2502   \u2502               |         attribution.txt\n\n[...]\n</code></pre>"}]}